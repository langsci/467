\documentclass[output=paper,colorlinks,citecolor=brown]{langscibook}
\ChapterDOI{10.5281/zenodo.17523034}
\author{Oliver Czulo\orcid{}\affiliation{Institut für Translatologie gGmbH} and        Ekaterina Lapshinova-Koltunski\orcid{}\affiliation{Universität Hildesheim}}
\title{Korpustranslatologie}
\abstract{Dieses Kapitel stellt die theoretischen Ansätzen des Funktionalismus, der Descriptive Translation Studies und der Universalientheorie als Motivation und als theoretische Grundlage dar, sich mit Translationsdaten in Form von Korpora auseinanderzusetzen. Es wird vor allem auf die produktorientierte Perspektive in der translatologischen Korpusforschung eingegangen. Ein methodisches Kapitel führt Grundbegriffe der Korpusanalyse ein und stellt beispielhaft Top-down- und Bottom-up-Zugänge in der Korpusarbeit dar. Prozessorientierte Perspektiven -- mit Blick auf kognitionsorientierte oder informationstheoretische Forschung -- werden ebenfalls aufgegriffen.}

\IfFileExists{../localcommands.tex}{
   \addbibresource{../localbibliography.bib}
   \input{../localpackages}
   \input{../localcommands}
   \input{../localhyphenation}
   \boolfalse{bookcompile}
   \togglepaper[23]%%chapternumber
}{}

\begin{document}
\maketitle 
\section{Einführung}
Die im späteren 20. Jahrhundert aufgekommenen funktionalen Translationstheorien \citep[u. a.][]{nord_loyalitat_1989,reiss_grundlegung_1984,holz-manttari_translatorisches_1984} lösten die Dichotomie der \glqq{}freien\grqq{} vs. der \glqq{}nahen\grqq{} Übersetzung auf und schlugen stattdessen ausgehend vom Übersetzungsauftrag -- der den Zweck und das Zielpublikum definiert -- ein Spektrum an Strategien vor, die sich zwischen den beiden genannten Polen bewegen können. Die Beschaffenheit des \textit{Translats}, d.h. des Produkts eines Translationsprozesses, hängt somit nicht allein vom Ausgangstext ab, sondern ist von verschiedenen Faktoren und den damit zusammenhängenden strategischen Erwägungen abhängig. Mit dieser Sichtweise rückt die Frage ins Blickfeld, welche Faktoren die Beschaffenheit des Translats wie beeinflussen, wobei inzwischen neben dem Übersetzungsauftrag zahlreiche andere situative Variablen wie z. B. das Kompetenzniveau der Translatoren, das Trägermedium des Translats etc. von Interesse sind.

Es sind vor allem die Ausführungen von \citet[insbes.][]{toury_descriptive_1995} und \citet[insbes.][]{baker_corpus-based_1996} sowie die zunehmende Verfügbarkeit von Sprachdaten, die in den 1990er Jahren die Entstehung einer Korpustranslatologie anstoßen. Anfangs konzentrierten die Untersuchungen sich auf die Frage, welche Faktoren translations\–spezifisch, oder genauer gesagt übersetzungsspezifisch sind, also was Übersetzungen von Nicht-Übersetzungen systematisch unterscheidet. Steht zu Beginn das (geschriebene) Produkt im Vordergrund, werden nach und nach Methoden – korpusbasierte wie andere – entwickelt, um auch Auswirkungen verschiedener Variablen auf den Übersetzungsprozess zu untersuchen.

Vergleichbare Untersuchungen setzen fürs Dolmetschen später als fürs Übersetzen ein. Zum einen liegt dies darin mitbegründet, dass gesprochensprachliche Daten nicht so leicht verfügbar sind wie geschriebensprachliche Da\–ten: Erstere müssen erst transkribiert werden.\footnote{Jüngste Fortschritte in der automatischen Transkription können hier Abhilfe schaffen.} Zum anderen kann man vermuten, dass ein hinderlicher Umstand war, dass das Dolmetschen nicht in gleicher Weise in den Wissenschaftsinstitutionen verankert ist wie das Übersetzen: Dolmetschprofessuren etwa sind deutlich seltener zu finden. In jüngerer Vergangenheit weitet sich der Blick über das Geschriebene und Gesprochene hinaus auch auf multimediale Aspekte der Translation. Neben den Forschungsinteressen spielen didaktische und praktische Aspekte, d.h. der Einsatz von Korpora als Wissensquelle in der Lehre und in der Translationspraxis, früh eine Rolle in der Korpustranslatologie.

\section{Theoretische Grundlagen der Korpustranslatologie}

Die Korpustranslatologie bedient sich in vielerlei Hinsicht der Grundlagen und Erkenntnisse der Korpuslinguistik\footnote{Eine eingehende theoretische sowie praktische Einführung in die Korpuslinguistik findet sich in \citep{lemnitzer_korpuslinguistik_2015}.} und ist stark empirisch geprägt. Sie passt somit zur Kategorisierung der \textit{Translation Studies} von \citet{holmes_name_2000} als empirische Wissenschaft.

%\subsection{Ausgangspunkte der Korpusforschung in der Translatologie}

\citet{biber_corpus_1998} zufolge besteht der Korpusansatz darin, Gebrauchsmuster in natürlich vorkommenden Texten zu analysieren. Grundlage ist dafür eine Sammlung von Texten, die man als \textit{das Korpus} bezeichnet. Die Analyse bedient sich der Unterstützung von Computern und bringt Beobachtungen \begin{itemize}
    \item auf der \textit{qualitativen} Ebene, also anhand von Einzelbeispielen, die meist tiefgehend analysiert werden, oft mit dem Ziel, daraus verallgemeinerbare Kategorien abzuleiten, und
    \item auf der \textit{quantitativen} Ebene, also anhand einer Vielzahl von Einzelbeobachtungen, die anhand bestehender Kategorisierung eingeordnet, ausge\–zählt und interpretiert werden,
\end{itemize} hervor. Dabei schließen diese beiden Ebenen einander nicht aus: Qualitativ hergeleitete Kategorien lassen sich quantitativ auf ihre Anwendbarkeit und Aussagekraft überprüfen; ebenso natürlich Kategorien, die quantitativ, z. B. über statistische Rechenverfahren, gewonnen wurden. Untersucht man quantitative Befunde, können Fundstellen auffallen, die nicht oder nicht so richtig in eine bestehende Kategorisierung passen; diese muss dann auf Grundlage qualitativer Analysen überarbeitet werden. Legt man sich nicht von vorneherein ausschließlich auf eine der beiden Herangehensweisen fest, wird sich so ein wechselseitiges, einander ergänzendes Verfahren ergeben.

Viele korpusbasierte Studien im Bereich der Translatologie folgen der Annahme, dass übersetzte Texte bestimmte sprachliche Gemeinsamkeiten auf\-wei\-sen, die nicht oder nur in geringerem Maße in nicht übersetzten Originaltexten vorkommen. Es gibt verschiedene Namen für diese übersetzungsspezifische Phänomene. Mona Baker spricht von Translationsuniversalia und meint dabei, dass diese vom Einfluss der spezifischen Sprachpaare unabhängig sind~\citep[243]{baker_corpus_1993}. \citet{toury_descriptive_1995} postuliert zwei \glqq{}Gesetze\grqq{} der Übersetzung, wobei er diese ausdrücklich als probabilistische Tendenzen ansieht. Das \textit{law of growing standardization} sagt aus: \glqq{}in translation, textual relations obtaining in the original are often modified [...] in favour of (more) habitual options offered by a target repertoire\grqq{} \citep[268]{toury_descriptive_1995}: Übersetzungen orientieren sich demzufolge eher als Originaltexte an etablierten Normen in Grammatik und Lexik der Zielsprache. Dem Gegenüber steht das \textit{law of interference} \citep[275]{toury_descriptive_1995}, demzufolge Eigenschaften des Ausgangstexts tendenziell in den Zieltext übertragen werden. Diese beiden scheinbar widersprüchlichen \glqq{}Gesetze\grqq{} bestimmen in einer Wechselwirkung die sprachlichen die Eigenschaften des übersetzten Texts mit. \citet{baker_corpus-based_1996} führt auf Grundlage von Vorläuferstudien vier Universalien auf (s. u.), die in der Folge vielfach erforscht, teils angezweifelt und später um weitere Universalien erweitert wurden. \citet{chesterman_beyond_2004} klassfiziert die Universalien in S- und T-Universalien, d.h. Übersetzungsuniversalien, die entweder von der Ausgangs- oder Zielsprache abhängig sind. Während die S-Universalien Wechselwirkungen zwischen Übersetzungen und ihren Ausgangstexten erklären sollen, beziehen sich die T-Universalien auf Unterschiede zwischen Übersetzungen und Originalen in der Zielsprache. Die ursprünglich von Baker aufgeführten Universalien sieht \citet{pym_tourys_2008} alle als Varianten von Tourys \textit{law of growing standardization} an, sie würden somit nach Chesterman unter die T-Universalien fallen.

Eine weitere Bezeichnung dieser Phänomene ist \textit{Translationese}, ein Begriff, der von~\citet{gellerstam_translationese_1986} geprägte wurde. Translationese ließe sich ins Deutsche mit \glqq{}Übersetzungssprech\grqq{} oder \glqq{}übersetzungstypischer Sprache\grqq{} übersetzen: Dies drückt pointiert aus, dass Übersetzungen ihren ganz eigenen Sprachstil haben, was sie, so betonen auch einige Forscher, zu einem Gegenstand echten Forsschungsinteresses jenseits einer reinen \glqq{}Defizitdiskussion\grqq{} \textendash{} also in einfachen Worten gesagt der oft gestellten Frage, was in der Übersetzung \glqq{}verloren geht\grqq{} \textendash{} macht. Zu den am häufigsten erforschten Phänomenen gehören:
\begin{itemize}
    \item \textit{Shining-through}~\citep{teich_cross-linguistic_2003} bzw. \textit{Durchscheinen}, also die Tendenz, die Häufigkeit von Ausgangstextmerkmalen zu reproduzieren, anstatt den Konventionen der Zielsprache zu folgen; 
    \item \textit{(Über-)Normalisierung}~\citep{baker_corpus_1993,baker_corpus-based_1996}, d. h. die Tendenz, sich an die typischen Merkmale der Zielsprache anzupassen und diese sogar übertrieben häufig zu verwenden; 
    \item \textit{Explizierung}~\citep{baker_corpus-based_1996, olohan_reporting_2000}, die Tendenz, die Inhalte deutlicher auszudrücken, anstatt sie implizit zu belassen; 
    \item \textit{Simplifizierung}~\citep{toury_descriptive_1995}, die Tendenz, die bei der Übersetzung verwendete Sprache zu vereinfachen und eindeutiger zu machen. 
\end{itemize}
%\subsection{Aktuelle Schwerpunkte // Forschungsfragen und Korpustypen}
%Korpusbasiert aber nicht nur deskriptiv auf die Translationsprodukte, sondern auch kognitive Aspekte, z.B. mithilfe von Informationstheorie. Diskurstranslatologie
Nicht alle Phänomene sind als rein sprachsystembezogen zu verstehen: Phänomene wie z. B. eine Tendenz zur Simplifizierung können anderweitige Ursache haben, etwa unterschiedliche Textsortenkonventionen oder kognitive Entlastungseffekte beim Übersetzen.

Diese Übersetzungsphänomene werden im Translationsprodukt anhand von Beobachtungen über die Verteilung (\textit{Distribution}) spezifischer sprachlicher Merkmale untersucht.  Da\-für werden verschiedene statistische Methoden und Techniken eingesetzt, die weiter unten in den Abschnitten~\ref{ssec:topdown} und~\ref{ssec:bottomup} beschrieben werden. %Erklärungstendenzen im Verhalten von Übersetzern zu verknüpfen.
Anhand dieser Distributionen lassen sich  übersetzte Texte von original verfassten Texten sogar automatisch unterscheiden~\citep{volansky_features_2015,baroni_new_2006}.

Die Phänomene wurden in den letzten Jahren in verschiedenen Kontexten untersucht, wie z. B. in verschiedenen Textsorten~\citep{evert_impact_2017,lapshinova-koltunski_exploratory_2017}, für verschiedene Modalitäten wie geschrieben und gesprochen~\citep{lapshinova-koltunski_found_2021,he_interpretese_2016,kajzer-wietrzny_interpreting_2012} für verschiedene Produzierenden wie z. B. professionelle Übersetzende, Novizen oder Laien~\citep{bizzoni_measuring_2021,kunilovskaya_lexicogrammatic_2020,popovic_differences_2020,de_sutter_towards_2017,rubino_information_2016} und verschiedene Übersetzungsmethoden wie automatisch, händisch oder kombiniert~\citep{popovic_computational_2023,konovalova_man_2022,van_der_werff_automatic_2022,vanmassenhove_machine_2021,culo_contrasting_2017}.

\section{Grundbegriffe und Methoden der Korpusanalyse}
\subsection{Grundbegriffe}
Ein Korpus wird von \citet[][39]{lemnitzer_korpuslinguistik_2015} als \glqq{}Sammlung schrift\-licher oder gesprochener Äußerungen in einer oder mehreren Sprachen\grqq{} definiert. Die Grenze zwischen einem Korpus und einer Datensammlung ziehen Lemnitzer und Zinsmeister anhand des Kriteriums, dass Korpora ganze Texte oder große Textausschnitte umfassen; allerdings betonen sie, dass diese Grenze nicht absolut definierbar sei. Sprechen wir heute über ein Korpus, meinen wir oft eine digitalisierte Sammlung. Bei der Zusammenstellung eines Korpus sind verschiedenste Kriterien denkbar, wobei einige Kriterien recht grundlegend für die wahrscheinlich meisten Korpora sind, insbesondere: 
\begin{itemize}
    \item \textit{Repräsentativität}: Ein Korpus soll ausreichend Material enthalten, um davon verallgemeinerbare Aussagen ableiten zu können. Diese Größe ist allerdings nicht fest bestimmbar, d. h. man kann keine absoluten Zahlen angeben, ab wann ein Korpus repräsentativ ist, zumal verschiedene Phänomene unterschiedlich häufig auftreten. Hier kommen oft Erfahrungswerte zum Einsatz, oder man kann ein Korpus sukzessive erweitern und dabei überprüfen, ob Beobachtungen, die man zuvor gemacht hat, auch auf das größere Korpus zutreffen. 
    \item \textit{Balanciertheit} (also \textit{Ausgewogenheit}): Ein Korpus soll Texte aus ausreichend verschiedenen Quellen umfassen, um verallgemeinerbare Aussagen machen zu können. In einem Korpus beispielsweise, das Texte von frühen Lernern einer Sprache enthält, um deren typische Fehler zu untersuchen, sollten nicht nur Texte einer oder sehr weniger Personen enthalten sein, da dies keine Verallgemeinerung auf weitere Personen erlauben würde.\footnote{Dieses Kriterium gilt natürlich dann, wenn es dem Ziel der Untersuchung entspricht. Anders würde es sich verhalten, würde man beispielsweise den Schreibstil nur einer Person in einem Genre untersuchen wollen.}
\end{itemize}

Korpora können nach \citet{lemnitzer_korpuslinguistik_2015} drei verschiedene Datentypen enthalten. 
\begin{itemize}
    \item \textit{Primärdaten}: Dies ist das Material, anhand dessen bzw. über das wir Erkenntnisse gewinnen wollen, z. B. ein übersetzter Text, ein transkribiertes Gespräch oder eine Videoaufnahme.
    \item \textit{Annotationen}: In unserem Fall sind dies linguistische Beschreibungen von Einheiten in den Primärdaten, z. B. das Labeln von Wortarten oder die Markierung von Sprechpausen.
    \item \textit{Metadaten}: Diese beschreiben kontextuelle Aspekte der Entstehung der Primärdaten, also z. B. das Alter der Textproduzenten oder die Entstehungszeit von Texten.
\end{itemize}

Es gibt eine ganze Reihe von Korpustypen, etwa Fachsprachenkorpora, Korpora von Sprachenlernern oder multimediale Korpora. Oft lässt sich ein Korpus mehreren Typen zuordnen, z. B. ist ein Korpus mit Videoaufnahmen, in denen Grundbegriffe der Physik erklärt werden, ein multimediales Fachsprachenkorpus. In einem solchen Korpus könnte neben den zu Hilfe genommen Bildern auch interessant sein, mit welchen Gesten bestimmte Aussagen gemeinsam vorkommen und ob sich zwischen Gesten und Gesagtem ein Zusammenhang erkennen lässt. So könnte man z. B. hypothetisieren, dass ein \glqq{}entweder [...], oder [...]\grqq{} durch markante Handbewegungen begleitet wird, die relativ zu zwei verschiedene Positionen vor dem Körper der Sprecher \textendash{} einmal links, einmal rechts der Körpermitte \textendash{} erfolgen.

Für die Translatologie sind zwei Arten von Korpora von besonderem Interesse: das \textit{Parallelkorpus} und das \textit{Vergleichskorpus}. Der Klassifikation von \citet[][]{granger_corpus_2003}\footnote{Die Begriffe \textit{Parallelkorpus} und \textit{Vergleichskorpus} wurden und werden teils noch heute uneinheitlich verwendet, so gibt es Autoren, die die Benennungen genau umgekehrt verwenden wie hier dargestellt. Die Granger'schen Definitionen haben sich inzwischen aber erfahrungsgemäß weitestgehend etabliert.} folgend gelten folgende Definitionen: 
\begin{itemize}
    \item \textit{Parallelkorpus}: Eine Sammlung von Originaltexten und deren Übersetzungen, bei denen jedem Originaltext mindestens eine Übersetzung zugewiesen ist.
    \item \textit{Vergleichskorpus}: Das mehrsprachige Vergleichskorpus ist eine Sammlung von Originalen und/oder Übersetzungen in mehreren Sprachen aus demselben Genre. Das einsprachige Vergleichskorpus enthält aus derselben Sprache Originale und Übersetzungen oder Erstsprachlertexte\footnote{\glqq{}Erstprachler\grqq{} oder wissenschaftlich oft \glqq{}L1-Sprecher\grqq{} werden umgangssprachlich als \glqq{}Muttersprachler\grqq{} bezeichnet, wobei ja nicht ausgemacht ist, dass dies die Sprache einer Mutter eines Textproduzenten ist. Hier meint Erstsprache die im gegebenen Kontext stärkste Sprache von Textproduzenten, d. h. nicht notwendigerweise die Sprache, die zuerst gelernt wurde.} und Lernertexte. Im ersteren Fall sind die Übersetzungen nicht notwendigerweise Übersetzungen der Originaltexte.
\end{itemize}

Um Übersetzungsbeziehungen zwischen sprachlichen Einheiten oder die Effekte der Ausgangssprache (nach Chesterman S-Universalien) zu untersuchen, ist ein Parallelkorpus erforderlich. Zum Beispiel kann ein beobachteter ungewöhnlich hoher Anteil an Nomen in übersetzten Texte ein Effekt der Ausgangssprache sein, die einen höheren Anteil an Nomen verwendet als in der Zielsprache üblich ist (also ein Durchscheinen). Andererseits kann der erhöhte Anteil an Nomen auch durch Überanpassungen an Konventionen der Zielsprache verursacht sein (also eine Normalisierung, nach Chesterman in der Gruppe der T-Universalien), was wiederum mit Hilfe monolingualer vergleichbarer Korpora zu überprüfen wäre. Darüber hinaus könnte es sich auch um ein \glqq{}drittes Phänomen\grqq{} handeln, also eines, das nicht von Ausgangs- oder Zielsprache abhängig ist. Dies könnte beispielsweise ein kognitiver Effekt des Übersetzungsprozesses sein, dass etwa Übersetzende tendenziell auf Nomen zurückgriffen, weil es ihnen kognitiv einfacher fiele, die Formulierungen so zu verfassen. Um letztere Annahme genauer zu beleuchten, genügen rein produktbasierte Korpora nicht, hierfür sind Prozesskorpora nötig, die Aspekte des Übersetzungsprozesses enthalten.

%\todo{Evtl. sind die Beispiele hier drin zu simpel?}
Die Korpustranslatologie geht in der Regel deskriptiv vor und beschreibt Ge\-brauchs\-muster in Texten. Dies kann entweder, wie eben dargelegt, hypothesengeleitet (\textit{top-down}) erfolgen oder auch datengeleitet (\textit{bottom-up}) ablaufen. Die beiden Herangehensweisen können kombiniert werden.

\subsection{Top-down-Analyse}\label{ssec:topdown}
Bei einer Top-down-Analyse werden ein existierendes theoretisches Modell oder Ergebnisse und Beobachtungen aus vorhergehenden Studien als Ausgangspunkt genutzt. Darauf aufbauend werden Hypothesen gebildet, die mittels Tests verworfen oder bestätigt werden können. Um sie zu testen, müssen Hypothesen \textit{operationalisiert} werden: Es müssen beobachtbare Phänomene benannt werden, deren Erscheinungsbild als \textit{abhängige Variable} von einer oder mehrerer \textit{unabhängigen Variablen} beeinflusst wird. Üblicherweise formuliert man eine so genannte \textit{Nullhypothese}, die keinen Zusammenhang zwischen unabhängigen und abhängigen Variablen sieht. Die \textit{Alternativhypothese} geht im Gegensatz dazu von einem Zusammenhang zwischen den unabhängigen Variablen und den abhängigen Variablen aus. Weiterhin müssen die sprachlichen Merkmale definiert werden, die für ein Phänomen stehen und in Korpora gesucht werden können. Die beobachtete(n) Ausprägung(en) der sprachlichen Merkmale werden (ggf. statistisch) ausgewertet und in Bezug auf Hypothesen oder Fragestellungen interpretiert.

Zum Beispiel wird in Arbeiten im Bereich der kontrastiven Pragmatik~\citep{house_translation_1997} ausgesagt, dass die deutsche Sprache inhaltsorientierter (unpersönlicher im Stil) als die englische Sprache ist, welche adressatenorientierter (durch persönlichere Ansprache) funktioniert. Auf diesen Beobachtungen aufbauend könnte man eine Hypothese aufstellen, dass Übersetzungen aus dem Deutschen ins Englische aufgrund von Einflüssen der Ausgangssprache inhaltsorientierter als vergleichbare Originale im Englischen sind. Hier wären die unabhängigen Variablen also die sprachlichen Konventionen der Ausgangs- und der Zielsprache sowie die Übersetzungsrichtung; die Häufigkeit der Produktion von inhalts- vs. adressatenorientierten Merkmalen wären davon abhängig. Die Nullhypothese würde besagen, dass kein Unterschied in der Häufigkeit unpersönlicher vs. persönlicher Ausdrücke zwischen übersetzten Texten und Originaltexten im Englischen bestünde; die Alternativhypothese würde unsere eben formulierte Annahme enthalten, dass die aus dem Deutschen übersetzten englischen Texte unpersönlicher gestaltet wären.

Um dies korpusbasiert zu untersuchen zu können, muss zunächst festgelegt werden, welche sprachlichen Ausdrücke im Deutschen und Englischen jeweils als inhalts- bzw. adressatenorientiert gelten. Die Inhaltsorientierung spiegelt sich z. B. in der Verwendung von Passivkonstruktionen, \emph{zu}-Infinitiven und Reflexivverben wider. Also werden im Korpus die Häufigkeiten von Passiv- und Reflexivverben und \emph{zu}-Infinitiven ermittelt. Passivkonstruktionen im Englischen lassen sich beispielsweise als Abfolge von einem Hilfs- oder Modalverb in finiter Form (\emph{is, are, was, were, has, have, can} etc.), gefolgt von einem optionalen Adverb (\emph{always, recently}, etc.), gefolgt von einem optionalen Verb \emph{to be} im Gerundium oder als Partizip II (\emph{been, being}) und gefolgt von Partizip II eines beliebigen Verbes definieren: \emph{is always being done, has been asked, were written, could be clarified}.

Anhand der ermittelten Häufigkeiten dieser sprachlichen Merkmale kann untersucht werden, ob englische Übersetzungen aus dem Deutschen tatsächlich eine signifikant höhere Anzahl inhaltsorientierter Merkmale haben als die vergleichbaren Originale im Englischen. Trifft dies zu, wäre die aufgestellte Hypothese aber noch nicht bestätigt: Es muss zusätzlich noch im deutsch-englischen Parallelkorpus anhand der deutschen Originale überprüft werden, ob die unpersönlichen Ausdrücke tatsächlich aus dem Deutschen übersetzt wurden. Ist dies nicht der Fall, würde man nicht von einem Durchscheinen ausgehen, sondern könnte u. a. annehmen, dass eine zielsprachliche Konvention beim Übersetzen ungewöhnlich oft verwendet wurde, was eher dem Phänomen der Normalisierung zuzuordnen wäre.

%Daraus folgt, dass 
Anhand dieses Beispiels wird deutlich, wie ein theoriegetriebener bzw. hypothesengeleiteter Ansatz in der Korpustranslatologie eingesetzt wird: Bereits existierende Vorarbeiten und Theorien werden für die Hypothesenbildung genutzt, außerdem soweit angegeben auch als Informationsquelle für die zu untersuchenden sprachlichen Merkmale.

\subsection{Bottom-up-Analyse}\label{ssec:bottomup}

Bottom-up-Ansätze werden auch als \textit{datengesteuert} oder \textit{datengeleitet} bezeichnet. Bei diesen Ansätzen werden zunächst Beobachtungen festgehalten, die mit einem notwendigen Minimum an Konzepten auskommen. Für die Interpretation der Beobachtungen werden theoretische Ansätze verwendet oder gebildet. 

Bottom-up-Ansätze arbeiten mit so genannten \textit{shallow features}, d. h. oberflächlichen sprachlichen Merkmalen, die automatisch leicht zu ermitteln sind und keine besonders theorieabhängige Fundierung haben, wie etwa Wortarten.\footnote{Streng genommen ist auch die Definition von Wortarten durchaus theorieabhängig, aber es haben sich gerade im Bereich der Korpusanalyse einige Konventionen durchgesetzt, die als quasi \glqq{}theorieneutral\grqq{} gelten und aufgrund ihres häufigen Einsatzes Ergebnisse vergleichbar machen.} Schon eine einfache Statistik über die Verteilung von Wortarten kann informativ sein: In einem Bottom-up-Ansatz würde man z. B. für ein Vergleichskorpus zunächst die Häufigkeitsverteilung aller Wortarten in übersetzten und Originaltexten beschreiben, um dann Abweichungen in den Verteilungen genauer unter die Lupe zu nehmen. Anders als bei einem Top-down-Ansatz würde man nicht von vorneherein bestimmte Verteilungen vermuten. Daneben wird auch mit so genannten \textit{N-Grammen} gearbeitet, wobei \textit{Gramme} eine kontinuierliche Abfolge von Elementen meint und \textit{N} für eine beliebige Zahl steht. N-Gramme können auf der Grundlage verschiedener Größen berechnet werden, z. B. Wortformen, Wortarten oder \textit{Tokens} (d. h. alle sprachlichen Oberflächenelemente, also Wörter, Satzzeichen, Emojis, ...). Ein Satz wie
\begin{description}
    \item \textit{Ich bin ein Satz.}
\end{description}
würde folgende Tri-Gramme, also Abfolgen mit jeweils drei Elementen ergeben:
\begin{itemize}
    \item Wortformen: \textit{Ich bin ein} // \textit{bin ein Satz}
    \item Wortarten: Pronomen–Vollverb–Artikel // Vollverb–Artikel–Nomen
    \item Tokens: \textit{Ich bin ein} // \textit{bin ein Satz} // \textit{ein Satz .}
\end{itemize}
Solche N-Gramme wurden u. a. verwendet, um Unterschiede zwischen maschinell übersetzten und menschlich übersetzten Texten in der Verteilung von Wortarten zu ermitteln \citep[z. B.][]{toral_post-editese_2019}. Die Ergebnisse deuten darauf hin, dass diese unterschiedlich produzierten Texte tatsächlich verschiedene sprachliche Profile haben.

Im Gegensatz zum Top-down-Verfahren kommt bei einem Bottom-up-Verfah\-ren also die Beobachtung vor der theoretischen Einordnung. In gewisser Weise kann man eine Analogie zu jenen qualitativen Verfahren ziehen, bei denen Kategorien und Hypothesen erst aus der Beobachtung (von Einzelbelegen) heraus gewonnen und nicht vorab angenommen werden.

\section{Ausblick}

Dieser sehr komprimierte Einblick in die Grundlagen der Korpustranslatologie wird der Breite des Feldes eigentlich kaum gerecht. Nicht nur werden inzwischen deutlich elaboriertere statistische Modelle verwendet, um Produktdaten Einsichten in Übersetzungsphänomene zu entlocken, sondern es werden hierfür zunehmend Prozessdaten -- also z. B. Blickbewegungsdaten oder Tastaturanschlagsprotokolle -- ausgewertet. Nur angerissen wurde außerdem die Relevanz kognitiver Faktoren, deren Untersuchung Einblicke in die Verarbeitungsprozesse bei der Übersetzung geben, oder multimedialer Faktoren, die durch neue digitale Möglichkeiten an Wichtigkeit zunehmen; und mit diesen Faktoren hören die Möglichkeiten der Beschreibung translatorischer Kontexte nicht auf. Die korpusbasierte Erforschung von Dolmetschdaten konnte ebenfalls nicht ausgeführt werden. Neben Untersuchungen auf all diesen Feldern bietet die korpustranslatologische Literatur zahlreiche korpuspraktische und -didaktische Überlegungen, die ebenfalls gewinnbringend sind.

% Grundlegende linguistische Annotationen sind daher weiterhin erforderlich, um die zu analysierenden Korpora nutzen zu können. %Ein Beispiel für auf Token-Ebene berechnete N-Gramme.

% Darüber hinaus werden hier auch Merkmalskombninationen eingesetzt, wie semi-delexikalisierte Wort-Ngramme, wobei die Inhaltswörter maskiert werden. Ein Beispiel dafür wäre die Studie von~\citet{ZampieriLapshinova2015}, in der Sprachvariaton in Übersetzungen untersucht wird. Die Autorenschaft nutzt die Darstellung, bei der alle Nomina durch Platzhalter (PLH) ersetzt werden, um die sogenannte semi-delexikalisierte Textdarstellung zu erreichen.

%\begin{itemize}
%\item Die weltweiten Herausforderungen im Bereich der Energiesicherheit erfordern über einen Zeitraum von vielen Jahrzehnten nachhaltige Anstrengungen auf der ganzen Welt.
%\item Die weltweiten PLH im PLH der PLH erfordern über einen PLH von vielen PLH nachhaltige PLH auf der ganzen PLH.
%\end{itemize}

% Diese Darstellung liegt zwischen der vollständig delexikalisierten Darstellungen (Wortarten-Ngramme) und der vollständig lexikalisierten Darstellung (Wort-Ngramme), die alle Wörter im Text ohne jegliche Ersetzung verwendet. Die Grundidee von datengetriebenen Ansätzen ist die Strukturen zu finden, die durch existierende Theorien und Studien noch nicht erfasst und unbekannt sind. So nutzen~\citet{LapshinovaZampieri2018} automatische Textklassifikation, um humane und maschinelle Übersetzungen von einander zu trennen. Dabei ist die Autorenschaft nicht nur daran interessiert, mit möglichst hoher Präzision die Übersetzungen von einander zu trennen, sondern auch die Merkmale zu filtern, die besonders distinktiv für eine spezifische Textsorte bzw. Übersetzungsvariante (maschinell oder human) sind. Somit wird eine Liste von Wortarten-Ngramme erstellt, die typisch für Humanübersetzungen ist, eine für maschinelle Übersetzungen. Im Anschluss werden die Ngramme in Bezug auf existierende Theorien interpretiert. 

%Neben solchen einfachen Häufigkeitsverteilungstechniken können aber auch deutlich komplexere statisstische Mittel zum Zuge kommen, wie z. B. die Kullback-Leibler Divergence~\citep[KLD, ][]{Kullback1951}, d.h. die asymmetrische Variante der relativen Entropie. Relative Entropie formalisiert die Nichtübereinstimmung oder Divergenz zwischen zwei Modellen (oder zwei Korpora) in Informationsbits. Je größer die Divergenz zwischen den zwei Modellen, desto linguistisch unterschiedlicher sind sie. Im Hinblick auf Übersetzung kann man somit die spezifischen Merkmale der Übersetzung untersuchen, wenn diese auf der der Basis von original erfassten Texten modelliert wird, oder umgekehrt kann man die typischen Merkmale der original erfassten Texte untersuchen, wenn diese auf der Basis von Übersetzung modelliert werden. Somit lassen sich sprachliche Merkmale aufdecken, die für Übersetzungen bzw. original erfasste Texte spezifisch sind.

%Diese Methode wird von~\citet{Karakanta2021} für die Analyse von Übersetzungen im Vergleich mit den original erfassten Texten verwendet. In der Arbeit von~\citet{Przybyl2022} wird KLD für die Analyse typischer Merkmale verdolmetschter Texte im Vergleich mit schriftlichen Übersetzungen genutzt. 

%\section{Beispielstudien}
%\begin{table}
%\caption{Frequencies of word classes}
%\label{tab:myname:frequencies}
% \begin{tabularx}{.8\textwidth}{X rrrr}
%  \lsptoprule
%            & nouns & verbs  & adjectives & adverbs\\
%  \midrule
%  absolute  &   12  &    34  &    23      & 13\\
%  relative  &   3.1 &   8.9  &    5.7     & 3.2\\
%  \lspbottomrule
% \end{tabularx}
%\end{table}

%\is{Cognition} %add "Cogntion" to subject index for this page

%\ea
%\gll cogito                           ergo      sum\\
%     think.\textsc{1sg}.\textsc{pres} therefore \textsc{cop}.\textsc{1sg}.\textsc{pres}\\
%\glt `I think therefore I am.'
%\z
%\il{Latin} %add "Latin" to language index for this page


%\section*{Abbreviations}
%\begin{tabularx}{.5\textwidth}{@{}lQ@{}}
%... & \\
%... & \\
%\end{tabularx}%
%\begin{tabularx}{.5\textwidth}{@{}lQ@{}}
%... & \\
%... & \\
%\end{tabularx}

%\section*{Acknowledgements}
%\citet{Nordhoff2018} is useful for compiling bibliographies
%\section*{Contributions}
%John Doe contributed to conceptualization, methodology, and validation. 
%Jane Doe contributed to writing of the original draft, review, and editing.

\sloppy
\printbibliography[heading=subbibliography,notkeyword=this]
\end{document}
