\documentclass[output=paper]{langscibook}
\ChapterDOI{10.5281/zenodo.17523030}
\author{Maria Wolf\orcid{}\affiliation{} and Oliver Czulo\orcid{}\affiliation{Institut für Translatologie gGmbH}}
\title{Algorithmische Übersetzung: Die Ideengeschichte vor der Entwicklung der Maschinellen Übersetzung}
\abstract{Der Traum, Sprachbarrieren mittels berechenbarer Methoden zu überwinden, ist keine Erfindung des zwanzigsten Jahrhunderts: Solcherlei Ideen lassen sich aktuell bis ins neunte Jahrhundert zurückverfolgen. Die Ideengeschichte ist brüchig, doch gerade die Vielfalt der gewagten Perspektiven macht ihre Nachzeichnung lohnenswert. In diesem Beitrag geben wir einen Überblick über verschiedene Konzepte zur Sprachübertragung bis zur Mitte des zwanzigsten Jahrhunderts, als algorithmische Verfahren zur Übersetzung durch den Einsatz von Maschinen eine neue Qualität gewannen.}
\IfFileExists{../localcommands.tex}{
  \addbibresource{../localbibliography.bib}
  \input{../localpackages}
  \input{../localcommands} 
  \input{../localhyphenation} 
  \togglepaper[1]%%chapternumber
}{}

\begin{document}
\maketitle 
\shorttitlerunninghead{Die Ideengeschichte der Algorithmischen Übersetzung} %%use this for an abridged title in the page headers
\label{chap:wolf-czulo:algue}

\section{Einleitung} %1. /

In diesem Kapitel skizzieren wir die Ideengeschichte dessen, was wir als \textit{Algorithmische Übersetzung} verstehen: Verfahren mit einer festgelegten Abfolge von vordefinierten Arbeitsschritten, die dem Ziel dienen, eine Übersetzung eines natürlichsprachlichen Ausgangstexts zu erstellen. Die \textit{Maschinelle Übersetzung} sehen wir als Umsetzung algorithmischer Verfahren mittels (überwiegend) elektronischer Rechenwerke. Algorithmische Übersetzungsverfahren fassen wir in diesem Beitrag insofern eng, als dass sie Aspekte der Berechenbarkeit und Determiniertheit mindestens in Ansätzen in sich tragen. Wir grenzen sie somit von Modellen für Humanübersetzungsverfahren ab, wiewohl diese Grenzen selbstverständlich fließend sein können.

Die hier vorgestellten Ansätze datieren vor dem 20. Jahrhundert. Anhand der vorgestellten Ideen wird deutlich, was die Gelehrten antrieb und in welchem historischen Zusammenhang sie agierten: al\nobreakdash-Kindī übersetzte in Bagdad griechische Schriften ins Arabische, Ramon Llull missionierte im Mittelmeerraum, Gottfried Wilhelm Leibniz suchte nach einer Universalsprache, um die Begriffswelt der Menschen zu erschließen, und Johann Joachim Becher wollte den Handel mit fremden Ländern fördern. Die Autoren sind Vordenker des technologischen Fortschritts, den wir in den vergangenen Jahrzehnten erlebt haben. Allerdings ist die Ideengeschichte der Algorithmischen Übersetzung brüchig: Nicht immer beziehen sich die Autoren auf ihre zeitlichen Vorgänger, und auch heute wird in der Literatur zur Maschinellen Übersetzung erfahrungsgemäß eher selten Bezug auf historische Überlegungen genommen.

Einige der Verfahren sind eng verbunden mit der Kryptologie, wobei kryptographische Verfahren Sprache verschlüsseln und damit auf eine bestimmte Art und Weise kodieren, während kryptoanalytische (auch: kryptanalytische) Verfahren eine solche Kodierung zu entschlüsseln versuchen. Die von \citet{dupont_cryptological_2018} skizzierte Archäologie der Maschinellen Übersetzung umfasst solche Verfahren, die Übersetzung als Dekodierung und Rekodierung verstehen. Die Idee einer Interlingua sucht nach einem Universalcode, der zwischen allen Sprachen vermitteln kann und nicht von den Idiosynkrasien natürlicher Sprachen geplagt ist, während Ansätze zu Plansprachen die Universalisierung einer künstlich geschaffenen Sprache erreichen wollen. Den Wunsch, die Informationen eines Ausgangstextes vollständig aus ihm zu lösen und in der Zielsprache zu generieren, kennzeichnet \citet{stein_maschinelle_2009} als fortwährende Utopie, welche die Geschichte der Maschinellen Übersetzung seit dem 13. Jahrhundert prägt.

\largerpage
\section{Entschlüsselungsverfahren nach al-Kindī}

Die ersten Zeugnisse der Beschreibung von Rechenverfahren zur Übersetzung eines Textes liegen heute in der \textit{Sulaymannja Library} in Istanbul und sind kryptoanalytischer Natur \citep{dupont_cryptological_2018, strick_kindi_2009}. Im neunten Jahrhundert nach Christus wirkte Abū~Yaʿqūb ibn Ishāq al-Kindī (ca.~800–873~n.~C.) in Bagdad und entschlüsselte „schleierhafte“\footnote{Die arabische Schule habe Wörter als Schleier beschrieben, die die Verbindungen zwischen Gedanken und Verstehen verdeckten. Demnach sei Kommunikation der Vermittlungsprozess, um diese Schleier aufzudecken \citep[u. a.][]{dupont_cryptological_2018}.} Schriftstücke. Seine \textit{Abhandlung über die Entzifferung von verschlüsselten Botschaften} wurde im Jahr 1987 wiederentdeckt. Über al-Kindī (auch Al Kindus) ist bekannt, dass er im Haus der Weisheit\footnote{Von der arabischen Schule ist bekannt, dass sie sich mit Kryptologie beschäftigte und bereits eine feste Terminologie nutzte zur Unterscheidung zwischen Verschlüsselung und Entschlüsselung, zur Bezeichnung von Ausgangstexten, verschlüsselten Texte und Übersetzungen sowie zur Beschreibung von Methoden und Prozessen. \citet{wieber_kryptographie_1977} gibt einen Einblick in die algorithmischen Verfahren der damaligen Kryptographie.} „mit der Übersetzung wissenschaftlicher Schriften aus verschiedenen Kulturkreisen ins Arabische“ \citep{strick_kindi_2009} beauftragt wurde. Er habe sich dabei vor allem mit den philosophischen Schriften der Griechen beschäftigt, verstand aber selbst kein Griechisch, sondern überarbeitete und kommentierte Übersetzungen seiner Dragomane,\footnote{Das arabische Wort \textit{tarǧumān} habe nicht nur Übersetzende und Dolmetschende bezeichnet, sondern auch „Dechiffreure“ und „Codebrecher“ \citep[257]{wieber_kryptographie_1977}.} die für die Entschlüsselung der fremden Texte zuständig waren. Aufgrund seiner Beiträge zur Geometrie, Arithmetik und Logik und seiner Abhandlung \textit{Über den Intellekt} wird er heute als „erster Philosoph der arabischen Welt“ bezeichnet. Er verfasste auch medizinische Schriften und lehrte seine Schüler Rechenverfahren. Einige Schriften seien später ins Lateinische übersetzt worden, aber viele seiner Werke verloren gegangen \citep{strick_kindi_2009}.

Al-Kindī war Experte für kryptologische Methoden und linguistische Kenntnisse, die seinerzeit in der arabischen Schule gelehrt und angewandt wurden. Seine o.~g. kryptoanalytische Abhandlung untersucht die arabische Sprache anhand phonetischer und syntaktischer Merkmale sowie anhand der Häufigkeitsverteilung der Buchstaben. \citet[16]{schmeh_codeknacker_2006} beschreibt al-Kindīs Methoden als „Ersetzungsverfahren“ und „Umordnungsverfahren“, mit welchen Buchstabenhäufigkeiten analysiert, häufige Wörter erraten und Buchstabenpaare ausgezählt wurden.

\begin{quote}
„Eine Möglichkeit, eine verschlüsselte Botschaft zu entziffern, vorausgesetzt, wir kennen ihre Sprache, besteht darin, einen anderen Klartext in derselben Sprache zu finden, der lang genug ist, um ein oder zwei Blätter zu füllen, und dann zu zählen, wie oft jeder Buchstabe vorkommt. Wir nennen den häufigsten Buchstaben den »ersten«, den zweithäufigsten den »zweiten«, den folgenden den »dritten« und so weiter, bis wir alle Buchstaben in der Klartextprobe durchgezählt haben.
\end{quote}

\begin{quote}
Dann betrachten wir den Geheimtext, den wir entschlüsseln wollen, und ordnen auch seine Symbole. Wir finden das häufigste Symbol und geben ihm die Gestalt des »ersten« Buchstabens der Klartextprobe, das zweithäufigste Symbol wird zum »zweiten« Buchstaben, das dritthäufigste Symbol zum »dritten« und so weiter, bis wir alle Symbole des Kryptogramms, das wir entschlüsseln wollen, auf diese Weise zugeordnet haben.“ \citep[al-Kindī nach][35]{singh_geheime_2001}.
\end{quote}

Aus der Perspektive der Gelehrten dürfte damals oft nicht klar gewesen sein, ob das vorliegende Schriftstück in fremder Sprache, in verschlüsselter Sprache oder in beidem vorlag. Dazu kommt, dass es damals noch keine Einheitssprachen, sondern eine unvorstellbare Sprachvarietät gab. Verständlich also, dass al-Kindī nach definierten Rechenschritten suchte – nach einem Algorithmus, der ihm die Arbeit erleichtern würde. Seine statistischen Methoden, z.~B. die Auszählung des Vokal-Konsonanten-Verhältnisses, wurden noch Jahrhunderte später zur Feststellung der Ausgangssprache genutzt.\footnote{Ein Beispiel hierfür sind die kryptologischen Werke von Leon Battista Alberti. Ein weiteres Beispiel ist der vom Kryptologen William Friedmann Anfang der 1920er Jahre entwickelte Koinzidenzenindex \citep[vgl.][]{dupont_cryptological_2018}.}

Kryptoanalytische Verfahren und Übersetzung sind aufgrund der Natur ihrer Ausgangstexte nicht zu verwechseln. Die (metaphorische) Verbindung der beiden Disziplinen, die jeweils einen eigenen Gegenstand haben und demnach einen anderen Zweck verfolgen, besteht jedoch in der Anwendung von Verfahren, mit denen zunächst der „Code“ des Ausgangstextes bestimmt wird, um somit den Ausgangstext zu „entschlüsseln“ und den Zieltext zu erhalten.\footnote{Dafür argumentiert auch \citet{dupont_cryptological_2018}: Er findet den Ursprung der Maschinellen Übersetzung in kryptoanalytischen Verfahren und verbindet die Ideengeschichte von Entschlüsselung und Übersetzung: Beides „crackt“ einen unbekannten Text. DuPont veranschaulicht anhand kryptoanalytischer Verfahren, wie sehr die (gemeinsame) Geschichte durch die Spannung zwischen Rationalismus und Empirismus geprägt ist.} Die Ideengeschichte beider Disziplinen überlappt sich aufgrund gemeinsamer Methoden und der Entwicklung ihrer Automatisierung, die in Programmiersprachen zur Ansteuerung von Maschinen kulminiert.

\section{Die \textit{ars combinatoria} von Ramon Llull}

Der Universalgelehrte Ramon Llull (1232–1316, auch Raimundus Lullus) widmete sich der Christianisierung des Mittelmeerraums, warb für die Lehre der hebräischen und arabischen Sprache an westeuropäischen Universitäten und gilt heute als der Begründer der westeuropäischen Orientalistik. Llull entwarf eine Argumentationsmaschine, mit der er weltanschauliche Begriffe vom Lateinischen ins Arabische übertrug. Mit dieser mechanischen Übertragungshilfe habe er die Missionierung erleichtern wollen \parencites[u. a.][]{martiny_ramon_2018}[31]{zotter_parallele_2004}.\footnote{\citet{zotter_parallele_2004} vermutet, dass Llull eine mechanische Unterstützung für christliche Missionare im Sinn hatte, weil Juden und Araber ihm argumentativ überlegen erschienen. Die Macht der Argumentation habe er jedoch unterschätzt, denn er sei auf einer seiner Missionsreisen gesteinigt worden \citep{zotter_parallele_2004}.} Sie funktionierte durch übereinanderliegende Drehscheiben,\footnote{Auch die Kryptologie kennt mechanische Konstruktionen aus verschiedenen Scheiben, so funktionieren z.~B. die Alberti-Chiffren auf diese Weise \parencites{dupont_cryptological_2018}[19f.]{schmeh_codeknacker_2006}.} deren Inhalt miteinander kombiniert werden konnte. Der Auszug aus Llulls Werk (Abb.~\ref{fig:Wolf_Czulo__Abb-1}) zeigt Skizzen dieser Scheiben, auf denen Zeichen, Buchstaben und Begriffe abgebildet sind.

\begin{figure}
    \centering
    \includegraphics[width=.98\linewidth]{figures/Wolf_Czulo__Algorithmische Übersetzung/Abbildung_1_Llull_Ars_brevis.jpg}
    \caption{Die vier Figuren der Ars brevis von Ramon Llull}
    Quelle: \href{http://dfg-viewer.de/show/?tx_dlf\%5Bid\%5D=http\%3A\%2F\%2Fzimks68.uni-trier.de\%2Fstmatthias\%2FT1895\%2FT1895-digitalisat.xml&tx_dlf\%5Bpage\%5D=8&tx_dlf\%5Bdouble\%5D=0&cHash=f4e40988d0b7f70c9ccb59fa9a0e6493}{Stadtbibliothek und Stadtarchiv Trier, Trier Hs. 1895/1428}
    \label{fig:Wolf_Czulo__Abb-1}
\end{figure}

Die Idee der \textit{ars combinatoria} wird heute als herausragend angesehen, da sie Begriffen Zeichen zuweist und über eine Zeichenfolge bestimmte Argumente festhält. Der Inhalt der zu kombinierenden Begriffe in Llulls Kombinatorik beruhe auf grundlegenden Prinzipien, die den drei großen monotheistischen Religionen gemein sind, womit die „konfliktträchtige Tradition der Textinterpretation der jeweiligen heiligen Schriften (Bibel, Talmud, Koran)“ durchbrochen worden sei \citep{duda_anfange_2016}. \citet{pring-mill_mikrokosmos_2001} beschreibt den „Mikrokosmos“ von Ramon Llull und liefert eine ausführliche Darstellung des Inhalts der Scheiben und der Funktionsweise ihrer Kombinationen. Die Sekundärliteratur ist sich darüber einig, dass Llulls Argumentationsmaschine mehr als ein Übersetzungswerkzeug war und damit die Wahrheit der Menschen erklärbar werden sollte \citep[vgl. u. a.][]{duda_anfange_2016,martiny_ramon_2018}. Llull gilt als Vordenker der Informatik, da er versucht, die Gedankenwelt der Menschen in Grundbegriffe zu zerlegen und sie über einen Code miteinander zu kombinieren. Die Idee, Begriffe durch Buchstaben zu ersetzen, also sprachliche Inhalte durch Zeichenfolgen, ihre Kombination durch eine Kodierung festzuhalten und dann auch wieder zu entschlüsseln, gilt heute als der erste Versuch, Sprache mittels einer Maschine zu verarbeiten, denn dieses algorithmische Verfahren ist das Grundprinzip von Programmier- und Maschinensprachen.

\section{Leibniz und seine Universalsprache} %4. /

Wie sehr die algorithmischen Verfahren zur Übersetzung durch ihren historischen Kontext und damit auch durch bestimmte weltanschauliche und wissenschaftliche Desiderate geprägt sind, zeigt die Weiterentwicklung von Llulls \textit{ars combinatoria} durch Gottfried Wilhelm Leibniz (1646-1716).

\begin{quote}
„Wenn man Charaktere oder Zeichen finden könnte, die alle unsere Gedanken genauso rein und klar ausdrücken könnten, wie die Arithmetik Zahlen oder die Analytische Geometrie Linien ausdrückt, dann könnte man in allen Angelegenheiten, soweit sie dem rationalen Denken zugänglich sind, das tun, was man in der Arithmetik und Geometrie tut.“ \citep[90]{leibniz_fragmente_1960}
\end{quote}

Leibniz war überzeugt davon, dass nicht nur Sprachen, sondern letztendlich alles, was wir wahrnehmen, aus einzelnen Einheiten besteht (Monadentheorie\footnote{Leibniz‘ Monadologie von 1714 beruht auf der Theorie, dass die Welt aus \textit{Monaden} (griech. \textit{monas} = Einzelteil, Einzelnes, Substanz) zusammengesetzt ist, und erklärt seine Methode, Gegenstände, die er verstehen will, in ihre Einzelteile zu zerlegen und deren Zusammenhang zu erforschen. Die Vorstellung, dass die wahrgenommene Welt aus zusammenhängenden Einheiten bestehe, führte ihn zur Überlegung, dass also die wahrgenommene und gedachte Welt in Einzelteile zerlegbar und ihr Zusammenhang kodifizierbar sei.}) und sich deshalb durch eine Kodierung beschreiben lässt. Er erstellte ein duales Zahlensystem, weshalb ihm die Entwicklung der binären Kodierung zugeschrieben wird und er heute als Vordenker der Informatik gilt \citep{breger_leibniz_2009}. Als letzter Universalgelehrter\footnote{Leibniz lebte in einer Zeit des wissenschaftlichen Umbruchs: In den Geistes- und Naturwissenschaften kristallisierten sich einzelne Disziplinen heraus und es setzte sich die Auffassung durch, dass Zusammenhänge nicht getrennt von ihren Gegenständen vorliegen und durch eine allgemeine Formel erklärbar seien.}  verknüpfte er mathematische und logische Erkenntnisse mit religiösen Konzepten, wobei Wissenschaft für ihn eine Art Universalkenntnis war. So habe Leibniz das binäre Zahlensystem, bei dem alle Zahlen durch Eins und Null kodiert sind, als „Sinnbild der göttlichen Schöpfung“ \citep[387]{breger_leibniz_2009} beschrieben: Wenn die Eins „die Einheit oder das Eine“ und die Null „das Nichts oder den Mangel an Existenz“ darstelle, werde ersichtlich, dass „Gott oder die absolute Einheit […] alles aus dem Nichts“ erzeuge (ebd.).

Leibniz zählt zu den Vertretern der wissenschaftlichen Überzeugung, dass alle Sprachen auf einem gemeinsamen Code beruhen. So verfasste Athanasius~Kircher (1602-1680) auf der Suche nach einer Interlingua als sprachuniversalen Code die \textit{Polygraphia nova et universalis –} eine Plansprache, die den schriftlichen Austausch unter den Völkern ermöglichen sollte und durch die Geheimsprache von Johannes Trithemius (1462\nobreakdash-1516, \textit{Polygraphiæ libri sex}) inspiriert gewesen sei \citep{strasser_lingua_1988}. Leibniz‘ \textit{characteristica universalis} verfolge die Idee eines „Alphabets des menschlichen Denkens“, welches durch Grundbegriffe alle menschlichen Begriffe umfasse, womit „alle wahren Sätze“ mechanisch gebildet werden könnten \citep[56]{bedurftig_philosophie_2015}. Die erdachte Universalsprache sollte „alle Begriffe der Wissenschaften ausdrücken“ und der „Verständigung der Menschen aller Nationen“ dienen \citep[56]{bedurftig_philosophie_2015}.

\begin{quote}
„Und wenn dies geschieht [...], werden zwei Philosophen, die in einen Streit geraten, nicht anders argumentieren als zwei Rechenmeister. Es genügt, dass sie eine Feder in die Hand nehmen, sich vor ein Täfelchen setzen und zueinander sagen: ‚Calculemus!‘ (Rechnen wir!)“ \citep[7: 198f.]{leibniz_philosophischen_1890}.
\end{quote}

Hier wird deutlich, dass die Universalsprache nicht ausschließlich als Übersetzungswerkzeug konzipiert wurde, sondern dass durch sie algorithmische Verfahren möglich werden sollten, mit denen die Arbeit der Wissenschaft erleichtert und die Wahrheit zugänglicher würde. Leibniz war überzeugt, dass es „unwürdig“ sei, die Zeit der Gelehrten mit einfachen Rechenaufgaben zu verschwenden und dass mit Maschinen solche Aufgaben schneller und fehlerfrei gelöst werden könnten. So entwarf er unter anderem seine \textit{machina deciphratoria} zur Verschlüsselung und Entschlüsselung von Botschaften.

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{figures/Wolf_Czulo__Algorithmische Übersetzung/Abbildung2_Sprossenrad_leibniz.png}
    \caption{Leibniz' Sprossenrad.}
    Quelle: \href{https://commons.wikimedia.org/wiki/File:Sprossenrad\_leibniz.png}{Wiki Commons}
    \label{fig:Wolf_Czulo__Abb-2}
\end{figure}

%[\textbf{Abbildung} \textbf{2}: Abbildung2\_Sprossenrad\_leibniz.png. Heruntergeladen von \url{https://commons.wikimedia.org/wiki/File:Sprossenrad_leibniz.png}, dort auch Infos zur Weiterverwendung, keine bessere Auflösung vorhanden.]

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{figures/Wolf_Czulo__Algorithmische Übersetzung/Abbildung3_Leibniz_Rechenmaschine_(1690).jpeg}
    \caption{Leibniz' Rechenmaschine.}
    Quelle: \href{https://de.wikipedia.org/wiki/Rechenmaschine\#/media/Datei:Leibniz\_Rechenmaschine\_(1690).jpg}{Wikipedia} 
    \label{fig:Wolf_Czulo__Abb-3}
\end{figure}

%[\textbf{Abbildung} \textbf{3:} Abbildung 3\_Leibniz\_Rechenmaschine\_(1960).jpg. Heruntergeladen von \url{https://de.wikipedia.org/wiki/Rechenmaschine#/media/Datei:Leibniz_Rechenmaschine_(1690).jpg}, dort Infos zur Weiterverwendung, 8,4 MB, andere Größen vorhanden.]

Seine Entwürfe für Rechenmaschinen waren für die Entwicklung der Mechanik zur Lösung von mathematischen Problemen ein gewaltiger Fortschritt. Grundlage hierfür war die Erfindung der Staffelwalze, deren mechanisches Grundprinzip in der handschriftlichen Skizze des Sprossenrads (siehe Abb.~\ref{fig:Wolf_Czulo__Abb-2}) festgehalten ist. Die Rechenmaschine auf Abbildung~\ref{fig:Wolf_Czulo__Abb-3} ist ein Originalentwurf von 1690. Seinerzeit fehlte es noch an geeigneten Baumaterialien als technische Voraussetzung für solche feinmechanischen Apparaturen und die Umsetzung seiner Entwürfe bedurfte der Entwicklung und Forschung im Bereich der Feinmechanik.

\section{Bechers Programmierversuch} %5. /

Zum Lebenswerk von Johann Joachim Becher (1635-1682) gehört die Entwicklung einer Interlingua als Code zur Entschlüsselung fremder Sprachen. Bechers Wirken als Chemiker, Techniker und Politikberater, seine Schriften über Handel und Warenexport und die Gründung einer Art Technologiezentrum für verschiedene Handelszweige verdeutlichen den gesellschaftlichen Kontext seiner Idee. \citet{sprengler_johann_2014} bezeichnet ihn als ersten Volkswirt, der als Merkantilist die Produktion und den Export von Waren fördern wollte. Er sei selbst kein Kaufmann gewesen, sondern habe sich theoretisch mit den Voraussetzungen für den Verkauf von Gütern in andere Länder beschäftigt \citep{sprengler_johann_2014}. Becher selbst sah sein Werk als Beitrag zur Völkerverständigung. Seine Methode zur Kommunikation in anderen Sprachen versah er mit dem Untertitel „Eine geheimschriftliche Erfindung, bisher unerhört, womit jeder beim Lesen in seiner eigenen Sprache verschiedene, ja sogar alle Sprachen, durch eintägiges Einarbeiten erklären und verstehen kann“ \citep[22. Übersetzung des lateinischen Originaluntertitels, 59]{becher_zur_1962}.

Sein Werk \textit{Allgemeine Verschlüsselung der Sprachen} (lat. Originaltitel: \textit{Character, pro Notitia Linguarum Universali}) aus dem Jahre 1661 wird heute als der erste Programmierversuch bezeichnet. Becher kannte Kirchers Entwurf einer Plansprache und entwarf ein Sprachsystem mit über zehntausend Wörtern, die er in einem erweiterbaren Indexverzeichnis festhielt. Die Zahlencodes wurden als Punkte und Striche in „Schlüsseln“ (siehe Abb.~\ref{fig:Wolf_Czulo__Abb-4} und~\ref{fig:Wolf_Czulo__Abb-5}) graphisch umgesetzt. Abbildung~\ref{fig:Wolf_Czulo__Abb-4} zeigt das Grundprinzip zur Entschlüsselung, dem beim Lesen dieser Zeichen gefolgt werden muss:

\begin{quote}
„Wenn du im Raum von A oder B, C, D ein kleines Strichlein in waagerechter Richtung erblickst, so bedeutet dies fünf. Einzelne Pünktchen bei A bezeichnen jedesmal eine Einheit, so oft sie gesetzt sind. Senkrechte Striche in B erfordern, daß du soviele Zehnereinheiten zählst, wie Striche da sind, in C soviele Hunderteinheiten wie Striche, in D soviele Tausender wie Striche vorhanden sind.“ \citep[31]{becher_zur_1962}
\end{quote}

In den Bereichen A, B, C und D (siehe Abb.~\ref{fig:Wolf_Czulo__Abb-4}) werde ein Wort anhand des Index dargestellt. Der zweite Teil des Schlüssels in den Bereichen E, F, G, H und I zeige die „Flexion des Wortes“ \citep[32]{becher_zur_1962}. In den Bereichen L und K können Satzzeichen markiert und Pluszeichen ergänzt werden. So markiere z.~B. ein Pluszeichen die adverbiale Verwendung eines Wortes, drei Pluszeichen bedeuten hingegen, dass es sich um „eine reine Zahl“ handelt \citep[32]{becher_zur_1962}. Abbildung~\ref{fig:Wolf_Czulo__Abb-5} ist der Abhandlung von \citet{reinermann_automatische_2006} entnommen und veranschaulicht die Bechersche Methode an einem Beispielsatz. Aufgrund der Komplexität des entwickelten Zeichensystems wird Becher, trotzdem sich seine Methode aufgrund vieler praktischer Mängel nicht durchsetzen konnte, als ein Vordenker der automatischen Sprachübersetzung angesehen \citep{reinermann_automatische_2006}.


\begin{figure}
    \centering
    \includegraphics[]{figures/Wolf_Czulo__Algorithmische Übersetzung/Abbildung4_Becher_Schlüsselprinzip_1661_ausschnitt_aus_Seite35.png}
    \caption{Das Becher'sche Schlüsselprinzip \citep[basierend auf][35]{becher_zur_1962}}
    \label{fig:Wolf_Czulo__Abb-4}
\end{figure}

%[Hier \textbf{Abbildung} \textbf{4} einfügen: Abbildung4\_Becher\_Schlüsselprinzip.png. Scan aus \citealt{Becher1962} (s.~u.), Ausschnitt aus Seite 35, selbst ausgeschnitten und Helligkeit und Kontrast optimiert. Buch liegt bei mir.]

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/Wolf_Czulo__Algorithmische Übersetzung/Abbildung5_Becher_Beispiel_Reinermann2014mod.png}
    \caption{Rechenbeispiel nach Becher \citep[basierend auf][18]{reinermann_automatische_2006}}
    \label{fig:Wolf_Czulo__Abb-5}
\end{figure}
%[Hier \textbf{Abbildung} \textbf{5} einfügen: Abbildung5\_Becher\_Beispiel\_Reinermann. Bild 14 entnommen aus \citealt{Reinermann2014}, Seite 18 (\url{https://www.uni-speyer.de/fileadmin/Ehemalige/Reinermann/jjbheft22.pdf}),


%[(optional auch \textbf{Abbildung} \textbf{6} einfügen\textbf{:} Abbildung6\_Becher\_Titelblatt\_1661. Scan aus \citealt{Becher1962} (s~u.), Seite 5. Bild ausgeschnitten und Kontrast optimiert, Buch liegt bei mir.)]

\section{Der Übergang zur Maschinellen Übersetzung} %6. /

Das 20. Jahrhundert markiert den Übergang zur Theorie und Praxis der Maschinellen Übersetzung, einem konzeptuell zwar untergeordneten, aber doch sehr starken und somit für sich selbst stehenden Feld. Zweifelsohne hat sich mit der Maschinellen Übersetzung ein Traum der alten Gelehrten erfüllt, denn sie selbst verbrachten viel Zeit damit, die Schriften unbekannter Sprache zu entschlüsseln, um deren Inhalt zu verstehen. Jede der hier vorgestellten Ideen für algorithmische Übersetzungsverfahren diente bestimmten gesellschaftlichem Zwecken und ist durch ihren philosophisch-weltanschaulichen Kontext geprägt.

Mitte der 1930er Jahre entwickelten Georges Artsrouni in Frankreich und Petr Trojanskij in der Sowjetunion unabhängig voneinander Übersetzungsmaschinen und meldeten dafür auch Patente an \citep{hutchins_two_2004}. Die von ihnen entwickelten algorithmischen Verfahren wurden auf bilinguale Wortverzeichnisse angewandt und konnten einfache Wortfolgen übertragen. Trojanskijs Überlegungen gingen wohl deutlich weiter als die von Artsrouni: Sein Übertragungsmechanismus basierte auf logischen Sinneinheiten und er konzipierte die Idee eines dreischrittigen Prozesses bestehend aus der Analyse von Wortformen in der Ausgangssprache, der Übertragung durch ein bilinguales Lexikon und der Synthese von Wortformen in der Zielsprache. Die Arbeiten dieser beiden Pioniere wurden allerdings im weiteren Verlauf kaum bis gar nicht rezipiert.

Erst das berühmte Weaver-Memorandum \citep{weaver_translation_1949} stieß die Forschung zur Maschinellen Übersetzung im großen Stil an. Die in der Folge entwickelten Ansätze nahmen in ihrer Mächtigkeit stetig zu. Sie werden heute üblicherweise in regelbasierte und datenbasierte Verfahren unterteilt. Blickt man aber auf die hier vorgestellten historischen Skizzen zurück, ließe sich durchaus eine andere Unterteilung vornehmen: Regelbasierte sowie einfache statistische Ansätze, die mit Übersetzungstabellen arbeiten, könnte man demnach unter Verfahren der Dekodierung und Rekodierung fassen, während neuronale Ansätze mit häufig in sich mehrsprachigen Sprachmodellen der Idee einer Universalcode-Maschine durchaus nahekommen.

Die Aufstellung der historischen Verfahrensskizzen für eine Algorithmische Übersetzung ist mehr als nur ein Zurückblicken auf die Anfänge. Neben dem Angebot einer alternativen Perspektive auf die Klassifikation heutiger Maschineller Übersetzungssysteme erinnert sie uns daran, wie alt und vielfältig der Traum von einer einfachen, berechenbaren Übersetzung zwischen Sprachen ist.


\sloppy
\printbibliography[heading=subbibliography,notkeyword=this]
\end{document}
