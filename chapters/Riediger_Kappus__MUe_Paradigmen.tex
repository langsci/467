\documentclass[output=paper,colorlinks,citecolor=brown]{langscibook}
\ChapterDOI{10.5281/zenodo.17523038}
\author{Hellmut Riediger\affiliation{Civica Scuola Interpreti e Traduttori Altiero Spinelli} and Martin Kappus\affiliation{Zürcher Hochschule für Angewandte Wissenschaften}}
\title{Moderne MÜ: Grundfunktionsweisen und Paradigmen}
\abstract{Maschinelle Übersetzung (Machine Translation, MT) ist eines der zentralen Themen der digitalen Translatologie und beeinflusst in zunehmendem Maße sowohl die Übersetzungsindustrie als auch die linguistische Forschung. Seit ihren Anfängen in den 1940er Jahren hat sich die MT von einfachen, regelbasierten Ansätzen zu hochkomplexen neuronalen Modellen entwickelt, die heute in der Lage sind, Übersetzungen in Echtzeit zu liefern. Diese Entwicklung stellt nicht nur eine technologische Erfolgsgeschichte dar, sondern auch einen tiefgreifenden Wandel in der Art und Weise, wie Übersetzung wahrgenommen, gelehrt und praktiziert wird. Dieses Kapitel bietet eine Einführung in die grundlegenden Funktionen moderner MT-Systeme und beleuchtet die Paradigmen, die ihre Entwicklung geprägt haben. Von regelbasierten und statistischen Methoden bis hin zu neuronaler maschineller Übersetzung (NMT) und großen Sprachmodellen (LLMs) werden die theoretischen und praktischen Grundlagen der Technologie skizziert. Darüber hinaus wird der Einfluss von MT auf die Rolle menschlicher Übersetzer*innen sowie auf traditionelle Übersetzungskonzepte wie Äquivalenz und kulturelle Adaption diskutiert. Ein besonderer Schwerpunkt liegt auf den Herausforderungen und Möglichkeiten, die sich aus der Integration von MT in hybride Übersetzungsprozesse ergeben. Post-Editing, Pre-Editing und die Interaktion mit Computer-Assisted Translation (CAT)-Werkzeugen sind zu unverzichtbaren Bestandteilen professioneller Übersetzungsarbeit geworden. Gleichzeitig werfen diese Entwicklungen Fragen zu Qualität, Ethik und Didaktik auf. Ziel dieses Kapitels ist es, ein fundiertes Verständnis der technologischen, methodischen und praktischen Grundlagen der modernen maschinellen Übersetzung zu vermitteln und deren Auswirkungen auf die Übersetzungspraxis und -theorie aufzuzeigen. Es richtet sich damit sowohl an Forschende und Studierende der Translationswissenschaft als auch an Praktiker*innen, die sich mit den Herausforderungen und Potenzialen der digitalen Übersetzung auseinandersetzen.}

%move the following commands to the "local..." files of the master project when integrating this chapter
\usepackage{tabularx}
\usepackage{langsci-optional}
\usepackage{langsci-gb4e}
%\bibliography{localbibliography} 

\IfFileExists{../localcommands.tex}{
   \addbibresource{../localbibliography.bib}
   \input{../localpackages}
   \input{../localcommands}
   \input{../localhyphenation}
   \boolfalse{bookcompile}
   \togglepaper[23]%%chapternumber
}{}

\begin{document}
\maketitle

\section{Der Traum der Überwindung von Sprachbarrieren }

Die Wurzeln der maschinellen Übersetzung (Englisch: \textit{Machine Translation} - kurz: MT) reichen weit in die Geschichte zurück. Bereits im Mittelalter träumten Denker von Systemen zur Überwindung von Sprachbarrieren. Ramon Llull entwickelte im 13. Jahrhundert mit seiner \textit{Ars Magna} ein logisches System zur Wissensrepräsentation, das als Vorläufer der Künstlichen Intelligenz (KI) gesehen werden kann. Sein Ziel war es, durch logische Kombination von Begriffen eine universelle Methode zur Erkenntnisgewinnung zu schaffen, wobei er noch keine direkte Anwendung auf die Sprachübersetzung im Sinn hatte \citep{koetsier_art_2016}.
Im 17. Jahrhundert griff Gottfried Wilhelm Leibniz die Idee einer symbolischen Darstellung von Wissen auf und entwickelte die \textit{characteristica universalis}, eine Universalsprache, die komplexe Gedanken in symbolische und formale Konzepte zerlegte \citep{sarayani_bestmogliche_2023}. Diese sollte als universelles Kommunikationsmittel in den Wissenschaften dienen, wobei der Gedanke einer universellen Struktur von Sprachen eine entfernte Verbindung zur späteren Entwicklung der maschinellen Übersetzung aufwies.
Johann Joachim Becher verfolgte ähnliche Ziele und arbeitete an einem numerischen System zur Übersetzung zwischen Sprachen \citep{hutchins_two_2004}. Obwohl sein Ansatz innovativ war, blieb er weitgehend unbeachtet und hatte keinen direkten Einfluss auf die späteren Entwicklungen der maschinellen Übersetzung.

Diese frühen Konzepte legten nicht unmittelbar die Basis für die maschinelle Übersetzung, boten aber wichtige theoretische Grundlagen in der Logik und Systematisierung von Wissen. Sie inspirierten spätere Fortschritte in der formalen Logik und der Computerlinguistik. Ihr Vermächtnis spiegelt sich heute in der Entwicklung von KI-gestützten Übersetzungssystemen und Sprachmodellen wider, die den komplexen Anforderungen der globalisierten Kommunikation gerecht werden und die Herausforderungen präziser, kontextgerechter Übersetzungen adressieren. Eine ausführlichere Darstellung dieser Vorläufer der modernen maschinellen Übersetzung finden Sie im Beitrag \glqq{}Algorithmische Übersetzung\grqq{} (Kapitel \ref{chap:wolf-czulo:algue}) in diesem Band.  


\section{Erste Übersetzungsmaschinen }
1933 entwickelten der Franzose Georges Artsrouni und der Russe Peter Trojanskij unabhängig voneinander erste Ansätze für maschinelle Übersetzungssysteme \citep{hutchins_two_2004}. Artsrouni meldete 1933 ein Patent für einen mechanischen Übersetzer an, der die Form eines Papierstreifens hatte. Trojanskij patentierte ebenfalls 1933 eine Maschine, die mithilfe eines logischen Zwischencodes und zweier Personen, die jeweils die Quell- und Zielsprache beherrschten, Übersetzungen durchführte. Trojanskijs System war für die vielsprachige Sowjetunion gedacht und beruhte auf der Annahme, dass Sprachen einer gemeinsamen logischen Struktur folgen könnten – ähnlich der späteren Universalgrammatik von Noam Chomsky. Trojanskijs Tod und die Isolation der Sowjetunion verhinderten jedoch die Weiterentwicklung seines Projekts. 

In den 1940er Jahren führte das wachsende militärische Interesse zu einem Wechsel von mechanischen zu elektronischen Computern. John von Neumann entwickelte eine Architektur, die Programme und Daten im gleichen Speicher ablegen konnte, was die Verarbeitung symbolischer Daten und den Wechsel zwischen Codes ermöglichte. Auch die interlinguale Übersetzung wurde als eine spezielle Form dieses Codewechsels betrachtet, ein Konzept, das eng mit der Kryptografie verbunden war. Diese ist die Kunst und Wissenschaft der Verschlüsselung von Informationen, um sie vor unbefugtem Zugriff zu schützen. Ihre lange Geschichte reicht bis in die Antike zurück und hat sich als Schlüsseltechnologie in der Sicherheit und Kommunikation etabliert. Besonders im Zweiten Weltkrieg spielte die Kryptografie eine entscheidende Rolle: Alan Turing und sein Team entschlüsselten den Enigma-Code der Deutschen, was als Meilenstein in der modernen Kryptografie und Computerwissenschaft gilt. Als Mathematiker wie Warren Weaver und Claude Shannon sich für die maschinelle Übersetzung interessierten, sahen sie Parallelen zur Kryptografie. In seinem berühmten Memorandum skizzierte \citet{weaver_translation_1949} die Potenziale der maschinellen Übersetzung. Er verglich Sprachen mit einem verschlüsselten Code, der entschlüsselt werden könnte, und betonte die Ähnlichkeiten zwischen dem Übersetzungsprozess und der Dekodierung verschlüsselter Nachrichten.



\section{Regelbasierte Maschinelle Übersetzung (RBMT)}
\subsection{Erste Systeme und erste Begeisterung}
Die erste Welle der maschinellen Übersetzung (RBMT, Rule-Based Machine Translation) in den 1950er Jahren basierte auf der Idee, sprachliche Regeln explizit zu codieren und von Computern ausführen zu lassen. Diese Systeme verwendeten Lexika und komplexe Regelwerke, um Wörter und Sätze zwischen Sprachen zu übersetzen. Diese Regelwerke dienten dazu den Ausgangstext zu analysieren, grammatische Umformungen vorzunehmen und schließlich zur zielsprachlichen Synthese. 1954 stellte IBM in New York ein erstes Übersetzungssystem vor, das als Demonstrationsprojekt diente. Es umfasste lediglich 250 Wörter und übersetzte 49 vordefinierte russische Sätze ins Englische \citep{hutchins_introduction_1992}. Dieses Ereignis schürte den Glauben, dass umfassende Übersetzungssysteme bald Realität sein könnten, was zu erheblichen Fördergeldern für die Forschung zur maschinellen Übersetzung durch die US-Regierung führte.
Diese Zeit war von großem Optimismus geprägt: Viele, darunter Leon Dostert, Direktor des Maschinellen Übersetzungsprojekts an der Georgetown University, glaubten an schnelle Fortschritte und daran, dass man bald über eine Technologie verfügen würde, die vollautomatische und qualitativ hochwertige Übersetzungen für viele Arten von Texten und viele Sprachkombinationen ermöglichen könnte.
Schnell stellte sich jedoch heraus, dass diese frühen Systeme nur einfache Sätze übersetzen konnten und zahlreiche Einschränkungen aufwiesen. So erkannte der österreichisch-israelische Linguist Bar-Hillel \citep{bar-hillel_present_1960} beispielsweise, dass das Scheitern der maschinellen Übersetzung auf einer reduzierten Auffassung von Sprache als rein logischem System beruhte. Er argumentierte, dass Maschinen aufgrund der inhärenten Mehrdeutigkeit und des komplexen Kontexts menschlicher Sprache nicht in der Lage seien, Semantik adäquat zu verarbeiten. Daher hielt er vollautomatische maschinelle Übersetzung für grundsätzlich unmöglich und plädierte stattdessen für \glqq{}halbautomatische\grqq{} oder \glqq{}menschlich unterstützte\grqq{} Übersetzungsmethoden.
Maschinelle Übersetzungssysteme stützten sich stark auf das saussurische Konzept der \textit{langue} \citep{de_saussure_cours_1916}, also das abstrakte, kollektive Sprachsystem mit grammatischen Regeln und Bedeutungen. In realen menschlichen Texten jedoch zeigt sich auch die \textit{parole}: die individuelle, kreative und oft einzigartige Anwendung von Sprache durch die Sprechenden. 1966 stellte der ALPAC-Bericht fest, dass die Fortschritte in der maschinellen Übersetzung weit hinter den Erwartungen zurückgeblieben waren, was zu einer drastischen Kürzung der Forschungsförderung führte  \citep{automatic_language_processing_advisory_committee_languages_1966}. Damit endete die optimistische Phase der frühen Forschung zur maschinellen Übersetzung.


\subsection{Krise der frühen maschinellen Übersetzung: Herausforderungen, praktische und wissenschaftliche Impulse}

Nach diesen ersten Rückschlägen in der maschinellen Übersetzung schwankte die Weiterentwicklung der Technologie zwischen Skepsis und Optimismus. Auf der einen Seite stand die Kritik von Yehoshua Bar-Hillel, der auf die Unfähigkeit von Maschinen hinwies, kontextabhängige Bedeutungen zu interpretieren. Diese Kritik wurde später in seinem Buch \textit{Language and information} \citep{bar-hillel_language_1964} systematischer ausgeführt und ist als das sogenannte \glqq{}Bar-Hillel-Paradoxon\grqq{} bekannt geworden.
Auf der anderen Seite dagegen blieb beispielsweise die sowjetische Computerlinguistin Izabella Bel’skaja \citep{belskaya_machine_1959} überzeugt, dass es bald möglich sein würde, alle Komponenten der Sprache, einschließlich der Semantik, zu formalisieren. Sie glaubte, dass maschinelle Übersetzungen für alle Textarten, sogar literarische, realisierbar wären, sobald die passenden Algorithmen entwickelt würden.
Ein großes Hindernis beim Fortschritt der maschinellen Übersetzung war die Trennung zwischen den technischen Disziplinen und den Geisteswissenschaften. Mathematiker*innen, Ingenieur*innen und Linguist*innen arbeiteten zwar gemeinsam an computergestützten Projekten, hatten jedoch oft nur wenig Verständnis für Sprachphilosophie, Semiotik oder Pragmatik. Dadurch konnte die menschliche Sprache – geprägt von Ambiguität und Kontextabhängigkeit – nicht vollständig erfasst werden. Dennoch legte diese Phase wichtige theoretische Grundlagen für spätere Entwicklungen in der maschinellen Übersetzung.

\subsection{Neuorientierung und Neuausrichtung}
Das Scheitern der ersten vollautomatischen maschinellen Übersetzungsversuche machte deutlich, dass linguistische Algorithmen weitaus komplexer waren, als ursprünglich angenommen. Dies führte zur Entstehung zweier wichtiger Forschungsrichtungen: Die erste Forschungsrichtung konzentrierte sich auf die Unterstützung menschlicher Übersetzer*innen durch technologische Hilfsmittel, die die Zusammenarbeit zwischen Mensch und Maschine verbesserten, um den Übersetzungsprozess effizienter zu gestalten. In den 1970er Jahren wurden die ersten elektronischen Terminologiedatenbanken entwickelt, und in den 1980er Jahren folgten computergestützte Übersetzungssysteme (CAT-Tools), mit Funktionen wie Translation Memories (TM), die menschliche Übersetzungen speicherten, um diese später wiederverwenden zu können.
Die zweite Forschungsrichtung beschäftigte sich mit den theoretischen Grundlagen der Übersetzung und trug maßgeblich zur Entstehung der Translationswissenschaft bei. Die ersten Versuche der maschinellen Übersetzung (MT) weckten ein großes Interesse an der Analyse des Übersetzungsprozesses. Obwohl die Übersetzungstheorie zunächst als Randgebiet der Linguistik galt, führten die Herausforderungen der interlingualen Kommunikation zu einer intensiven Suche nach effizienten Übersetzungsmethoden. Der Übersetzungsprozess wurde damals grundsätzlich als Dekodierungs- und Rekodierungsprozess verstanden, bei dem der Ausgangstext in seine Bestandteile zerlegt und im Zieltext wieder zusammengesetzt wurde. In den 1950er Jahren entstanden Modelle wie die Äquivalenztheorie, die das Verhältnis zwischen Quell- und Zieltext untersuchte. \citet{oettinger_new_1960} definierte das Übersetzen als die Umwandlung von Zeichen oder Repräsentationen in andere Zeichen und Repräsentationen, während \citet{catford_linguistic_1965} mit seinem Konzept der \glqq{}translation shifts\grqq{} (Übersetzungsverschiebungen) die Möglichkeiten und Grenzen sowohl der menschlichen als auch der maschinellen Übersetzung analysierte.
Mit der zunehmenden Diversifizierung der Übersetzungsindustrie in den 1980er und 1990er Jahren geriet das Äquivalenzparadigma jedoch unter Druck. Funktionalistische Theorien wie die Skopostheorie, die den Zweck und die Funktion des Zieltextes ins Zentrum rückte \citep{reiss_grundlegung_1984}, und der \glqq{}Cultural Turn\grqq{} \citep{bassnett_translation_2000}, der kulturelle Aspekte betonte, verschoben den Fokus weg von einer wörtlichen, prinzipiell ausgangstextorientierten Übersetzung. Übersetzer*innen mussten nun Entscheidungen treffen, die weit über die bloße sprachliche Wiedergabe hinausgingen, was ihre Rolle komplexer und verantwortungsvoller machte.

\section{Statistische Maschinelle Übersetzung (SMT)}
In den 1990er Jahren erlebte die maschinelle Übersetzung eine neue Ära der Begeisterung mit der Einführung statistischer Systeme, auch bekannt als Statistical Machine Translation (SMT). Diese Systeme verließen sich nicht mehr ausschließlich auf festgelegte Regeln, sondern nutzten umfangreiche Textkorpora, um statistische Modelle zu trainieren. Diese Modelle ermöglichten es, die Wahrscheinlichkeit von Wörtern oder Phrasen in der Zielsprache auf Grundlage der Ausgangssprache vorherzusagen.
Die Einführung statistischer Modelle führte zu erheblichen Verbesserungen der Übersetzungsqualität und stellte einen wichtigen Meilenstein in der Entwicklung der maschinellen Übersetzung dar. Besonders bei komplexeren Texten erzielten diese Systeme bessere Ergebnisse. Im Gegensatz zur regelbasierten Übersetzung, die auf festen sprachlichen Regeln basiert, orientierte sich SMT an empirischen Daten. Dies führte zu flexibleren und oft flüssigeren Übersetzungen, die besser mit der tatsächlichen Sprachverwendung übereinstimmten.
SMT verwendete sogenannte N-Gramme, also kleine Gruppen aufeinanderfolgender Wörter, um Muster in der Ausgangs- und Zielsprache zu identifizieren. Während des Trainingsprozesses wurde das System mit großen Mengen an Beispielsätzen gefüttert, die in N-Gramme unterteilt wurden. Der Lernalgorithmus ermittelte dann, welche N-Gramme in der Zielsprache am wahrscheinlichsten auftreten, basierend auf den entsprechenden N-Grammen in der Ausgangssprache.
Obwohl SMT keine tiefere linguistische Grundlage hatte und es ihr oft an innerer Logik und Konsistenz mangelte, erzielte sie dennoch überraschend gute Ergebnisse. Mit der Verfügbarkeit dieser Systeme im Internet, insbesondere durch Google Translate, wurden maschinelle Übersetzungen erstmals für eine breite Öffentlichkeit zugänglich. Diese Dienste konnten auf eine ständig wachsende Menge an online verfügbaren Texten zurückgreifen, was ihre Leistung weiter verbesserte.
Trotz dieser Fortschritte zeigte sich jedoch, dass eine vollständige Automatisierung des Übersetzungsprozesses mit hoher Qualität für alle Sprachkombinationen und Textarten weiterhin unerreichbar blieb. Statistische Ansätze erbrachten deutliche Verbesserungen, stießen jedoch an Grenzen bei komplexen Kontexten und semantischer Tiefe. Um diese Herausforderungen zu überwinden, wurden auch hybride Systeme entwickelt, die regelbasierte und statistische Ansätze kombinierten. Um die Übersetzungsqualität weiter zu verbessern, nutzten diese Systeme die Flexibilität der statistischen Modelle und kombinierten sie mit der Präzision regelbasierter Verfahren.

\section{Übersetzen mit KI}
\subsection{Neuronale Maschinelle Übersetzung (NMT)}
Seit Mitte der 2010er Jahre hat die neuronale maschinelle Übersetzung (NMT), die auf großen Sprachkorpora und modernen KI-Technologien basiert, die maschinelle Übersetzung grundlegend revolutioniert. Der Ausgangspunkt dafür war das erste neuronale Übersetzungssystem, das von \citet{wu_googles_2016} präsentiert wurde. Die von NMT-Systemen produzierten Texte sind deutlich flüssiger, natürlicher und lesbarer. Zudem erreichen sie auf Satzebene eine höhere sprachliche Korrektheit als frühere Übersetzungstechnologien.
NMT nutzt künstliche neuronale Netzwerke, ein Konzept aus der Künstlichen Intelligenz, das lose von der Funktionsweise des menschlichen Gehirns inspiriert ist. Diese Netzwerke bestehen aus vielen miteinander verbundenen \glqq{}Einheiten\grqq{} oder \glqq{}Knoten\grqq{}, die Informationen verarbeiten und gewichten. Sie sind in Schichten organisiert, durch die die Daten propagiert werden, damit das System eine Übersetzung erstellt. Um effektiv arbeiten zu können, wird das Netzwerk mit großen Mengen zweisprachiger Texte, den sogenannten Parallelkorpora, trainiert, um Muster und Beziehungen zwischen den Sprachen zu erkennen. Ein zentrales Konzept in der NMT ist die Encoder-Decoder-Struktur: Der Encoder analysiert den Eingabetext und wandelt ihn in eine kompakte, numerische Darstellung um, die als Vektor bezeichnet wird. Dieser Vektor ist im Grunde eine Reihe von Zahlen, die die wesentlichen Informationen des Textes zusammenfassen. Der Decoder verwendet diese Zahlenreihe, um den Text in die Zielsprache zu übersetzen. 
Ein weiterer wichtiger Bestandteil der NMT-Technologie ist der Attention-Mechanismus, der es dem Modell ermöglicht, sich auf besonders relevante Teile des Eingabetextes zu konzentrieren  \citep{vaswani_attention_2017}. Dieser Mechanismus verbessert die Genauigkeit und Natürlichkeit der Übersetzungen, da das Modell erkennt, welche Teile des Textes für die jeweilige Übersetzung besonders wichtig sind.
Das Training eines NMT-Modells erfordert große Rechenressourcen und viel Zeit, aber nach Abschluss des Trainings kann das Modell neue Texte effizient und genau übersetzen. Zu den Vorteilen von NMT-Systemen gehören eine höhere Genauigkeit, Flüssigkeit sowie die Fähigkeit zur kontinuierlichen Verbesserung durch die Integration neuer Daten. NMT-Modelle sind zudem flexibel und anpassbar an verschiedene Fachgebiete und Sprachdomänen. 
Es gibt jedoch auch Herausforderungen: Im Unterschied zu früheren Systemen wie der regelbasierten (RBMT) und der statistischen maschinellen Übersetzung (SMT) basiert die neuronale maschinelle Übersetzung (NMT) auf der Analyse ganzer Sätze, was zu einer verbesserten grammatikalischen und morphosyntaktischen Korrektheit führt. Dadurch wird die \glqq{}Fluency\grqq{} (Sprachflüssigkeit) der Übersetzungen deutlich verbessert. Diese Verbesserung betrifft jedoch nicht immer die \glqq{}Accuracy\grqq{} (Genauigkeit), also die genaue Wiedergabe der Bedeutung des Ausgangstextes. Aufgrund der höheren Lesbarkeit besteht die Gefahr, dass Benutzer*innen den Übersetzungen zu sehr vertrauen, insbesondere wenn sie weniger erfahren im Umgang mit sensiblen Texten sind. Dies kann problematisch sein, da fehlerhafte Übersetzungen bei wichtigen Dokumenten, wie rechtlichen, medizinischen oder technischen Texten, schwerwiegende Konsequenzen nach sich ziehen können.
Der hohe Rechenaufwand, die Abhängigkeit von großen Datenmengen sowie die Herausforderungen bei der Übersetzung seltener Wörter und weniger verbreiteter Sprachpaare stellen potenzielle Einschränkungen dar. Zudem sind NMT-Systeme oft intransparent, da es schwierig ist, nachzuvollziehen, wie das Modell zu bestimmten Entscheidungen gelangt. Während NMT besonders effektiv für häufig genutzte Sprachpaare ist, verlieren die Systeme bei weniger verbreiteten Sprachen oder spezialisierten Fachgebieten häufig an Übersetzungsqualität. 

\subsection{Large Language Models (LLMs)}
\label{riediger_kappus_subsec:llm}
Die jüngste Entwicklung in der maschinellen Übersetzung sind Large Language Models (LLMs) wie z.B. GPT-3, GPT-4 oder BERT, die mit Millionen bis Milliarden von Parametern Texte generieren und übersetzen. Diese Modelle, entwickelt von Unternehmen wie OpenAI und Google, zeigen beeindruckende Fähigkeiten in der Verarbeitung natürlicher Sprache.
Obwohl sowohl neuronale maschinelle Übersetzung (NMT) als auch LLMs auf großen neuronalen Netzwerken basieren, unterscheiden sie sich in ihrer Trainingsmethodik und ihren Anwendungsbereichen. NMT-Modelle werden direkt auf zweisprachigen Übersetzungskorpora trainiert, was sie speziell für das Übersetzen optimiert, jedoch oft ohne umfassende Berücksichtigung des sprachlichen Kontexts. Dadurch sind NMT-Systeme besonders geeignet für spezialisierte Anwendungsgebiete, bei denen Genauigkeit, Konsistenz und Datensicherheit von entscheidender Bedeutung sind. Beispiele hierfür sind Fachtexte, Unternehmensdokumente und datenschutzsensitive Inhalte, bei denen eine präzise, ausgangstextorientierte Übersetzung erforderlich ist.
LLMs hingegen werden zunächst auf Millionen von monolingualen Texten vortrainiert, wodurch sie ein breites Verständnis der Sprachen entwickeln, zwischen denen sie übersetzen. Darüber hinaus bieten LLMs zahlreiche zusätzliche Funktionen: Sie können Texte nicht nur übersetzen, sondern auch in verschiedenen Genres und Stilen verfassen, analysieren, zusammenfassen und stilistische Richtlinien befolgen. Diese Fähigkeiten ermöglichen es LLMs, flüssigere und kontextuell reichhaltigere Texte zu erzeugen. Allerdings zeigen sie in spezialisierten Bereichen oder bei anspruchsvollen Fachübersetzungen oft weniger Genauigkeit als NMT-Modelle.
Die Wahl zwischen NMT und LLM hängt stark von den spezifischen Anforderungen der jeweiligen Übersetzungsaufgabe ab. Während NMT für präzise und technisch exakte Übersetzungen bevorzugt wird, eignen sich LLMs besser für Aufgaben, bei denen Kreativität, Sprachfluss und Kontextverständnis eine größere Rolle spielen.

\section{Typologie von MÜ-Systemen}
Das Internet verbindet Menschen weltweit, doch eine der verbleibenden Barrieren bleibt die Sprachbarriere. Um diese Herausforderung zu überwinden, investieren große Unternehmen wie Google, Microsoft, Amazon, Apple, Yandex und Baidu erhebliche Ressourcen in die Entwicklung moderner Systeme für maschinelle Übersetzung (MT). Ihr Ziel ist es, Inhalte verständlich und die Kommunikation zwischen Nutzer*innen, Kund*innen und Unternehmen nahtlos zu gestalten.
Mehrheitlich greifen Web-Benutzer*innen auf diese Dienste zurück, um die maschinelle Übersetzungen für alltägliche Aufgaben zu gebrauchen. Online-Übersetzer wie Google Translate, DeepL, Lara, Bing oder Yandex sowie integrierte Übersetzungsfunktionen in Browsern und Anwendungen wie Microsoft Word erleichtern das Verstehen von Websites, Produktbeschreibungen und anderen Texten. Auch Sofortübersetzungsdienste in sozialen Medien wie Facebook oder LinkedIn sowie Tools zur Übersetzung von Videountertiteln und Texten in Bildern oder gesprochener Sprache sind weit verbreitet. Diese Technologien unterstützen nicht nur den Informationsaustausch, sondern auch das Sprachenlernen. 
Unternehmen und Fachleute setzen MT-Systeme gezielt ein, um qualitativ hochwertige Übersetzungen in Echtzeit zu erhalten. Die Einführung von neuronalen maschinellen Übersetzungsmodellen (NMT) hat die Übersetzungsqualität deutlich verbessert, insbesondere bei der Sprachflüssigkeit. Dadurch sind diese Systeme besonders hilfreich für Branchen wie Medizin, Recht und Technik, wo eine schnelle und präzise Verarbeitung spezifischer Inhalte entscheidend ist.
Die Vielfalt der maschinellen Übersetzung hat sich erweitert: Neben allgemeinen Systemen gibt es zunehmend maßgeschneiderte bzw. benutzerdefinierte, adaptive oder interaktive Übersetzungsmodelle, die an spezifische Anforderungen angepasst werden können. 
Dabei unterschieden sich die Systeme darin, ob und in welcher Phase der Entwicklung oder Nutzung Anpassungen gemacht werden können (O. Czulo p. K.). Ist das System eher statisch (also von der Trainingsphase geprägt) oder findet während der Nutzung (also im Übersetzungsprozess) ein Lernprozess statt und wenn ja, wie sieht dieser aus? Diese Entwicklungen fördern auch die Unterstützung weniger verbreiteter Sprachen und spezialisierter Fachgebiete. 
So lässt sich eine differenzierte Typologie von MT-Systemen erkennen, die sich darin unterscheidet, wie flexibel und anpassbar sie sind – sei es durch Training vor der Nutzung oder Lernen während des Einsatzes.

\subsection{Generalistische Systeme}
Bekannte generalistische Übersetzungssysteme wie DeepL, Google Translate und Microsoft Translator bieten online verfügbare Dienste für eine Vielzahl von Texten und Sprachpaaren. Diese Systeme nutzen umfangreiche, heterogene Datenmengen, um qualitativ hochwertige Ergebnisse für allgemeine Inhalte zu liefern. Dennoch stoßen sie bei fachspezifischen oder besonders anspruchsvollen Texten oft an ihre Grenzen, da sie nicht für spezifische Domänen oder Kontexte optimiert sind.
Kostenpflichtige Versionen dieser Dienste erweitern den Funktionsumfang erheblich. Sie ermöglichen unter anderem den Download bearbeitbarer Dokumente und bieten die Integration in Computer-Aided Translation (CAT)-Tools über APIs oder Plug-ins, was Übersetzungsprozesse effizienter gestaltet.
Ein häufiger Kritikpunkt bleibt jedoch die Verarbeitung der Daten auf externen Servern, was bei sensiblen oder vertraulichen Informationen problematisch sein kann. Anbieter wie DeepL bieten in kostenpflichtigen Versionen die Möglichkeit, Daten nach Abschluss der Übersetzung automatisch zu löschen, um den Datenschutz zu verbessern und den Einsatz in geschäftskritischen Bereichen zu erleichtern.

 

\subsection{Maßgeschneiderte MT-Engines}
Eine maßgeschneiderte MT-Engine wird durch Training mit unternehmensspezifischen Daten wie Translation Memories, Glossaren und zweisprachigen Textkorpora an die individuellen Bedürfnisse und Terminologien eines Unternehmens oder Fachbereichs angepasst. Dadurch können besonders präzise und konsistente Übersetzungen erstellt werden, die die spezifischen Anforderungen besser erfüllen als allgemeine Übersetzungssysteme. 
Das Training einer Übersetzungsengine erfordert jedoch umfangreiche zweisprachige Daten in hoher Qualität, wie zum Beispiel konsistente Translation Memorys oder parallelisierte Texte, um optimale Ergebnisse zu erzielen. Durch diesen Prozess können Unternehmen ihre Markenstimme, den gewünschten Stil und Ton sowie regionale Unterschiede in den Übersetzungen berücksichtigen. Dies ist besonders vorteilhaft bei hochspezialisierten Inhalten wie Marketingmaterialien, technischen Dokumentationen oder juristischen Texten. 
Eine Voraussetzung für erfolgreiches Training ist das Vorhandensein einer ausreichenden Datenmenge: Als Richtwert gelten mindestens 15.000 eindeutige Segmente, um eine zuverlässige Anpassung zu gewährleisten. Größere Mengen führen in der Regel zu noch besseren Ergebnissen. Zwar können die anfänglichen Trainingskosten hoch sein, doch der langfristige Nutzen – durch effizientere Prozesse und qualitativ hochwertige Übersetzungen – macht diese Investition besonders für Unternehmen mit regelmäßigem Übersetzungsbedarf lohnenswert. 
Maßgeschneiderte Systeme können weitreichend angepasst werden, erfordern jedoch erhebliche Datenmengen und initialen Aufwand. Für viele Unternehmen, die keine vollständige Neuentwicklung benötigen oder deren Datenbasis begrenzt ist, stellt sich die Frage nach einer flexibleren Lösung. Hier kommen benutzerdefinierte Systeme ins Spiel: Sie bieten einen Mittelweg zwischen generalistischen und vollständig maßgeschneiderten MT-Engines, indem sie auf bestehenden Modellen aufbauen und diese gezielt mit unternehmenseigenen Daten optimieren.

\subsection{Benutzerdefinierte MT-Engines}
Benutzerdefinierte Systeme wie Google Cloud AutoML, Microsoft Custom und Globalese ermöglichen die Anpassung von ursprünglich generischen MT-Systemen an spezifische Bedürfnisse, indem sie mit unternehmenseigenen Daten trainiert werden. Diese Systeme verbessern die Terminologie und stilistische Konsistenz und sind besonders effektiv bei fachspezifischen Texten. Trainingsdaten wie Translation Memorys (im TMX-Format) und Terminologiedateien (im TBX-Format) sind hierbei besonders wertvoll. Je mehr eigene Daten verwendet werden, desto präziser ist der Output. Solche benutzerdefinierten MT-Systeme lassen sich in gängige CAT-Tools integrieren, und Post-Editing-Ergebnisse können zur kontinuierlichen Verbesserung der Engine genutzt werden.
Diese Systeme ermöglichen es Organisationen, ihre Markenidentität zu bewahren und ihre eigene Terminologie sowie regionale Unterschiede in maschinellen Übersetzungen zu berücksichtigen. Durch die Verwendung von Glossaren und Do-Not-Translate (DNT)-Listen wird die Genauigkeit der Übersetzungen erhöht, was den Aufwand für Nachbearbeitungen erheblich reduziert. Dies ist besonders vorteilhaft für technologische oder detailorientierte Inhalte, die eine präzise Fachterminologie erfordern. Man kann darüber hinaus Input- und Output-Normalisierungsregeln anwenden, um Inkonsistenzen im Ausgangstext zu korrigieren und die Qualität der Übersetzung zu optimieren. Zwar verursacht die Anpassung solcher Systeme sowohl einmalige als auch laufende Kosten, doch bleibt sie wirtschaftlich günstiger als das vollständige Anlernen einer neuen MT-Engine, insbesondere für Unternehmen mit regelmäßigem Übersetzungsbedarf. 

\subsection{Adaptive und interaktive MT-Systeme}

Wenn nicht genügend eigene Daten verfügbar sind, können vortrainierte (generische) Systeme wie eTranslation der Europäischen Kommission domänenspezifische Übersetzungen liefern. Diese Systeme nutzen umfangreiche Sprachressourcen, die speziell für bestimmte Anwendungsbereiche, wie rechtliche oder administrative Texte, optimiert sind. Adaptive MT-Systeme, wie ModernMT, lernen in Echtzeit aus Benutzerkorrekturen und passen sich kontinuierlich an veränderte Anforderungen an. Dies macht sie besonders geeignet für dynamische Workflows, in denen Terminologie und Stil laufend aktualisiert werden müssen. Interaktive MT-Systeme, wie LILT, ermöglichen eine direkte Zusammenarbeit zwischen Mensch und Maschine. Benutzer*innen können Übersetzungsvorschläge in Echtzeit beeinflussen, während das System durch diese Interaktionen lernt und sich weiterentwickelt. Diese Kombination aus Automatisierung und menschlicher Kontrolle führt zu personalisierten und kontextuell präziseren Übersetzungen.



\section{Professioneller Einsatz von maschineller Übersetzung (MT)}
\largerpage
Die professionelle Übersetzung umfasst heute ein breites Spektrum von Tätigkeiten, die zwischen den Extremen der ausschließlich menschlichen Übersetzung einerseits und der ausschließlich maschinellen Übersetzung andererseits liegen. Dazwischen existieren verschiedene Kombinationen beider Ansätze (s. Abb.~\ref{riediger:abb:spektrum}). Daher kann die Arbeit mit oder für MT sehr unterschiedliche Ziele und Modalitäten haben.

\begin{figure}
\centering
\includegraphics[width=.8\linewidth]{figures/Riediger_Kappus__MUe_Paradigmen__Abb-1.png}
\caption{\label{riediger:abb:spektrum}Spektrum der Nutzung von MT}
\label{fig:enter-label}
\end{figure}

Nachfolgend sind die Dienstleistungen aufgeführt, die in der Post-Editing-Norm ISO 18587 \citep{iso_iso_2017} genannt werden. Diese Norm regelt den Einsatz von MT bei den Tätigkeiten von Sprachdienstleistern und Übersetzungsagenturen.
\subsection{Post-Editing}

\textbf{Post-Editing (PE)} oder \textbf{Post-Revision} bezieht sich auf die Nachbearbeitung von maschinell erstellten Übersetzungen (MT-Output). Dabei unterscheidet man zwischen:

\begin{description}
\item[Light Post-Editing (Leichtes Post-Editing):] Hierbei handelt es sich um eine menschliche Überarbeitung auf einem grundlegenden Niveau, oft als \glqq{}gut genug\grqq{} (good enough), \glqq{}den Zweck erfüllend\grqq{} (good for purpose) bezeichnet. Dies wird häufig für interne Kommunikation verwendet, bei der der Post-Editor nur Fehler korrigiert, die das Verständnis des Textinhalts beeinträchtigen.

\item[Full Post-Editing (Vollständiges Post-Editing):] Dies umfasst eine hochwertige menschliche Übersetzung (human quality) und Überarbeitung, z. B. von Verträgen, offiziellen Dokumenten, Verkaufsunterlagen oder zur Veröffentlichung bestimmte Texte. Die menschliche Übersetzer*in korrigiert alle sprachlichen Fehler und nimmt stilistische Anpassungen vor.
Ausführlichere Darstellungen verschiedener Aspekte des Post-Editing finden Sie in den Beiträgen \glqq{}Post-Editing Grundlagen\grqq{} und \glqq{}Post-Editing Kompetenzen\grqq{} in diesem Band.
\end{description}

\subsection{Pre-Editing}
Pre-Editing bezeichnet die Anpassung von Quelltexten \textit{vor} der maschinellen Übersetzung (MT), um Lesbarkeit und Verständlichkeit zu verbessern sowie den Nachbearbeitungsaufwand zu minimieren. Typische Maßnahmen umfassen die Korrektur von sprachlichen und formalen Fehlern, die Vereinheitlichung der Terminologie, die Vereinfachung grammatikalischer Strukturen und die Beseitigung von Mehrdeutigkeiten. Tools wie Acrolinx, Congree oder Large Language Models (LLMs) können diesen Prozess unterstützen.

Pre-Editing wird vor allem in zwei Fällen angewendet: zur Optimierung der Übersetzbarkeit von Texten für den Einsatz mit MT-Engines, um den Post-Editing-Aufwand zu reduzieren, und zur Aufbereitung von Trainingsdaten für das Training einer MT-Engine \citep{iso_iso_2017}. Nach ISO 18587 (2017) kann es auch während der Texterstellung mit kontrollierten natürlichen Sprachen (\textit{controlled natural language}, CNL) erfolgen, die Wortschatz, Grammatik und Stil einschränken, um die Übersetzbarkeit zu verbessern \citep{kuhn_survey_2014}.
 Klassische Pre-Editing-Maßnahmen, wie die Eliminierung von Mehrdeutigkeiten, die Vereinfachung von Satzstrukturen oder die Standardisierung von Terminologie, waren insbesondere bei regelbasierten und statistischen MT-Systemen essenziell. Bei modernen, KI-gestützten Übersetzungssystemen spielt Pre-Editing hingegen eine geringere Rolle. Dennoch bleibt die Forderung nach verständlichem und präzisem Schreiben zentral, um die Vorteile aktueller Technologien bestmöglich zu nutzen. Ein klar formulierter Ausgangstext führt in jedem Fall zu besseren Ergebnissen – unabhängig davon, ob KI im Übersetzungsprozess eingesetzt wird oder nicht. 

\subsection{Augmented Translation (Integration von CAT-Tools, MT und LLMs)}

Ein vorherrschender Trend in der professionellen Übersetzungsbranche ist die Konvergenz – die intelligente Kombination von Translation Memory (TM), maschineller Übersetzung (MT) und Large Language Models (LLMs). Auf professioneller Ebene wird MT zunehmend in Computer-Assisted Translation (CAT)-Tools integriert. \citet{kruger_augmented_2019} verwendet dafür den Begriff \glqq{}Augmented Translation\grqq{}. Viele führende CAT-Tools wie RWS Trados Studio, memoQ oder Phrase ermöglichen es, neben TM auch MT in den Übersetzungsprozess einzubeziehen.
Dies erlaubt Nutzer*innen, Dateien automatisch vorübersetzen zu lassen oder maschinelle Übersetzungsvorschläge während des Übersetzungsprozesses direkt in Segmente zu integrieren. Bearbeitete Segmente werden im TM gespeichert, was langfristig die Übersetzungsqualität verbessert.

Die Integration von MT in CAT-Tools erfolgt auf zwei Arten:
\begin{description}
  \item[Direkte Integration:] MT-Vorschläge werden direkt in den CAT-Workflow eingebunden. Tools wie Phrase (ex-Memsource) wählen mithilfe künstlicher Intelligenz automatisch die am besten geeignete MT-Engine für den jeweiligen Inhalt aus (Memsource, 2023).
  \item[Indirekte Integration:] Separate MT-Systeme werden genutzt, deren Ergebnisse anschließend in das CAT-System importiert werden.
\end{description}
Viele CAT-Tools bieten Plugins oder kostenpflichtige APIs an, um MT-Systeme nahtlos in den Übersetzungsprozess zu integrieren. Lösungen wie RWS Trados Studio, memoQ und Across Language Server ermöglichen damit einen effizienten und flexiblen Workflow für Übersetzungsprojekte.

 

\subsection{Weitere berufliche Anwendungen oder Dienstleistungen}

Neben Post-Editing und Pre-Editing umfasst der professionelle Einsatz von maschineller Übersetzung eine Vielzahl von Tätigkeiten und Dienstleistungen, die auf spezifische Branchen und Anwendungen zugeschnitten sind. Zu den wichtigsten gehören:

\begin{description}
  \item[Gisting:] Erstellung einer groben Übersetzung, um den allgemeinen Inhalt eines Textes schnell zu erfassen, ohne eine vollständige Übersetzung zu benötigen.
  \item[Anpassung und Training von MT-Engines:] Anpassung maschineller Übersetzungssysteme an Kundenbedürfnisse durch Training mit firmenspezifischen Daten wie Translation Memories und Terminologien (siehe oben).
  \item[Terminologiedatenbanken:] Erstellung und Pflege von Datenbanken mit spezifischen Begriffen und Übersetzungen zur Gewährleistung von Konsistenz und Qualität.
  \item[Beratung und Schulung:] Unterstützung bei der Implementierung von MT-Lösun\-gen und Schulungen zur effektiven Nutzung durch Mitarbeiter*innen.
\end{description}


\section{Qualitätsbewertungsmethoden und -metriken}
Die Frage nach der Qualität einer Übersetzung hängt stark vom Kontext ab. Für dynamische Inhalte wie Social Media, Kundenservice oder E-Mail-Kommunika\-tion reicht oft eine \glqq{}ausreichend gute\grqq{} (\textit{good enough}) Übersetzungsqualität. Für Texte, die veröffentlicht werden, wie Marketingmaterialien, Bedienungsanleitungen, Verträge oder Zeitschriftenartikel, ist hingegen eine hohe Qualität unerlässlich (\textit{human quality}).


\subsection{Menschliche Bewertungsmethoden}
Eine weitverbreitete Methode der menschlichen Bewertung ist das von \citet{koehn_statistical_2010} entwickelte Modell \glqq{}Adequacy-Fluency\grqq{}. Bewertet wird anhand zweier Kriterien:
\begin{description}
  \item[Adequacy (Angemessenheit):] Vermittelt der übersetzte Satz denselben Inhalt wie das Original? Gibt es Auslassungen, Ergänzungen oder Verzerrungen?
  \item[Fluency (Flüssigkeit):] Ist der Text in der Zielsprache grammatikalisch und stilistisch korrekt und flüssig?
\end{description}

Bewertet wird auf einer numerischen Skala von 1 bis 5, wobei 5 die beste Bewertung darstellt \citep{koehn_statistical_2010}.
Andere Skalen konzentrieren sich auf Verständlichkeit oder Nutzbarkeit, wie z.B. die Skala von \citet{arnold_machine_1994}, die zwischen perfekt verständlicher/nutzbarer, weitgehend verständlicher/nutzbarer, bedingt verständlicher/nutzbarer und überarbeitungsbedürftiger und unverständlicher Übersetzung unterscheiden.

Eine wesentliche Einschränkung solcher Bewertungsskalen liegt jedoch in ihrer Fokussierung auf einzelne Sätze. Mit der zunehmenden Qualität maschineller Übersetzungen werden Aspekte wie Kohärenz und Konsistenz auf Textebene immer relevanter, aber von diesen Skalen nur unzureichend berücksichtigt. Sie ignorieren Probleme auf Dokumentenebene, wie:
\begin{description}
  \item[Kohäsion:] Die sprachlichen Mittel, die zur Verbindung der einzelnen Teile eines Textes verwendet werden, wie z. B. Pronomen, Konjunktionen, Synonyme sowie sprachliche Bezugnahmen auf Begriffe (z. B. Hyponyme, Hyperonyme, Ellipsen).
  \item[Kohärenz:] Die logische und sinnvolle Verknüpfung von Ideen und Inhalten innerhalb eines Textes, die einen zusammenhängenden Gedankengang ermöglicht.
  \item[Konsistenz:] Die einheitliche und systematische Verwendung von Terminologien, Formulierungen und Übersetzungen, insbesondere bei wiederholten oder ähnlichen Inhalten.
\end{description}

\largerpage
Die von \citet{laubli_has_2018} vorgeschlagene Bewertungsskala auf Dokumentenebene bietet eine umfassende Methode zur Qualitätsbewertung maschineller Übersetzungen. Im Gegensatz zu satzbasierten Ansätzen berücksichtigt diese Skala nicht nur die Qualität einzelner Sätze, sondern auch die Kohärenz und Kohäsion eines gesamten Dokuments. Zu den bewerteten Kriterien gehören Kohärenz, Kohäsion, Grammatik, Stil, kulturelle Angemessenheit, Konsistenz und der Gesamteindruck des Textes. Besonders die Konsistenz, etwa in Terminologie und Stil, spielt eine entscheidende Rolle für die Einheitlichkeit des Textes. Die Integration dieser Kriterien in den Bewertungsprozess ermöglicht es Übersetzer*innen, sicherzustellen, dass ihre Übersetzungen sowohl grammatikalisch korrekt als auch auf die Zielgruppe abgestimmt sind. Insgesamt stellt diese Skala einen bedeutenden Fortschritt in der Qualitätsbewertung maschineller Übersetzungen dar.\smallskip

\textit{Vorteile:}
\begin{itemize}
    \item Sie nutzt die sprachliche und fachliche Expertise erfahrener Übersetzer*innen. 
\end{itemize}

\textit{Nachteile:}
\begin{itemize}
    \item Die Bewertung ist zeitaufwendig und kostenintensiv.
    \item Aufgrund der subjektiven Natur solcher Bewertungen sind mehrere Gutachter*innen erforderlich, was die Kosten weiter steigert. 
\end{itemize}

\subsection{Automatisierte Bewertungsmethoden}
Automatisierte Metriken wie BLEU, NIST, METEOR, TER oder BERT bewerten die Qualität maschineller Übersetzungen, indem sie diese mit Referenzübersetzungen vergleichen und Übereinstimmungen analysieren. Diese Metriken nutzen häufig die Levenshtein-Distanz, die die Anzahl der erforderlichen Änderungen misst, um den maschinell übersetzten Text in die Referenzübersetzung umzuwandeln \citep{papineni_bleu_2002}.\smallskip

\textit{Vorteile:}
\begin{itemize}
    \item Schnell und kosteneffizient.   
    \item Hilfreich bei der Entwicklung und Optimierung von  MT-Systemen.
\end{itemize}

\textit{Nachteile:}
\begin{itemize}
    \item Nicht geeignet für den praktischen Übersetzungsworkflow, da Referenzübersetzungen erforderlich sind.
    \item Berücksichtigt weder Kohäsion noch Kohärenz auf Dokumentenebene. 
\end{itemize}

\subsection{Qualitätsschätzung (\textit{Quality Estimation})}
Eine neuere Entwicklung in der automatisierten Qualitätsbewertung ist die Quality Estimation (QE), die auf künstlicher Intelligenz basiert. Dabei werden maschinelle Lernmodelle trainiert, um die Qualität von Übersetzungen ohne Referenztext zu beurteilen. Diese Modelle analysieren linguistische Merkmale wie Satzstruktur, Wortwahl und grammatikalische Korrektheit, um die erforderliche Nachbearbeitung einzuschätzen. Ein Beispiel dafür ist die Bewertungsskala des CAT-Tools Phrase Memsource, die maschinelle Übersetzungsoutputs in folgende Kategorien einteilt:\footnote{Diese Kategorien wurden inzwischen vom Phrase Quality Performance Score abgelöst, der Werte von 1--100\% vergibt}

\begin{description}
    \item[\textbf{100\%:}] Sehr gute Übereinstimmung der maschinellen Übersetzung; wahrscheinlich keine Nachbearbeitung erforderlich.
    \item[\textbf{99\%:}] Nahezu perfekt; eventuell geringfügige Nachbearbeitung nötig.
    \item[\textbf{75\%:}] Gut geeignet, erfordert jedoch wahrscheinlich eine Nachbearbeitung
    \item[\textbf{Keine Bewertung:}] Sehr wahrscheinlich von geringer Qualität; sollte nur als Referenz verwendet werden.
\end{description}

In jüngster Zeit werden auch Large Language Models (LLMs), wie GPT-Modelle, zur Unterstützung bei der Qualitätsschätzung eingesetzt. Der Leistungsumfang dieser Modelle sind in Abschnitt~\ref{riediger_kappus_subsec:llm} beschrieben. Trotz dieser Fortschritte bestehen weiterhin Herausforderungen bei der Implementierung von QE, etwa der hohe Bedarf an umfangreichen Datenmengen für das Training der Modelle und die Schwierigkeit, subtile kontextuelle Nuancen vollständig zu erfassen. 

\section{Fazit: Einfluss auf die Auffassung und die Didaktik der Übersetzung und die Rolle des Menschen}

Die rasante Entwicklung von Übersetzungstechnologien verändert traditionelle Vorstellungen von Übersetzung und der Rolle von Übersetzer*innen grundlegend. Übersetzungsarbeit erfolgt heute zunehmend in Zusammenarbeit mit maschinellen Systemen, die mittlerweile beeindruckende Leistungen erzielen und viele standardisierte Aufgaben schneller und zuverlässiger als durchschnittliche menschliche Übersetzer*innen bewältigen.  Dadurch verliert der Beruf der risikoarmen, gering qualifizierten \glqq{}Text- oder Wortersetzer*in\grqq{} an Bedeutung, da Technologien Geschäftsmodelle, Preise und Arbeitsprozesse umgestalten. Übersetzungsprofis, Agenturen und Projektmanager*innen müssen sich an diese neuen technologischen Realitäten anpassen. Wie etwa \citet{bernardini_toward_2021}, \citet{massey_human_2022} und \citet{riediger_traduzione_2023} betonen, wird die Rolle der menschlichen Übersetzer*innen zunehmend vielfältiger. Sie agieren als Expert*innen für Adaption, Post-Editing, Qualitätskontrolle, Transkreation, Sprachdatenmanagement und mehrsprachige Kommunikationsberatung.
Die Perfektionierung maschineller Übersetzungssysteme und deren Auswirkungen auf Übersetzungsprozesse, die Definition von Qualität sowie kulturelle und semantische Herausforderungen haben auch die Übersetzungstheorie nachhaltig beeinflusst. Traditionelle Konzepte wie die Äquivalenztheorie, die eine textuelle Entsprechung zwischen Ausgangs- und Zieltext anstrebt, werden von datengetriebene Ansätzen herausgefordert. Statistische und neuronale Modelle basieren auf probabilistischen Übereinstimmungen und sind auf syntaktische und lexikalische Muster ausgerichtet, ohne tiefere semantische oder kulturelle Nuancen vollständig zu erfassen. Während diese Systeme sprachlich präzise Übersetzungen generieren können, stoßen sie bei Kontext und Kultur an ihre Grenzen. 
Im Zeitalter von MT und KI gewinnen funktionalistische Ansätze wie die Skopostheorie an Bedeutung \citep[vgl.][]{reiss_grundlegung_1984}. Sie betonen, dass der Zweck einer Übersetzung zentral für deren Ausführung ist. Diese Perspektive ist besonders relevant für Texte, die über den bloßen Informationsaustausch hinausgehen, wie Marketing- oder juristische Inhalte, bei denen kulturelle Sensibilität und inhaltliche Präzision entscheidend sind. Die Transformation der Übersetzerrolle von Schöpfer*in hin zu Post-Editor*in hat tiefgreifende Auswirkungen auf Berufsbilder. Neben den sprachlichen Fähigkeiten benötigen Übersetzer*nnen heute technische Kompetenzen, um effektiv mit MT- und KI-Systemen zu arbeiten. Post-Editing und Qualitätssicherung werden zu zentralen Fähigkeiten, die in Ausbildungsprogrammen verstärkt berücksichtigt werden. Neue Modelle für die Mensch-Maschine-Interaktion sind erforderlich, um hybride Übersetzungsprozesse optimal zu gestalten. Zudem wirft der Einsatz von MT ethische Fragen auf: Wie können kulturelle Sensibilität, die Vermeidung von Stereotypen und die verantwortungsvolle Nutzung von Sprachdaten gewährleistet werden?  Übersetzer*innen, Entwickler*innen und Forscher*innen müssen Leitlinien entwickeln, um Verzerrungen und diskriminierende Inhalte in MT zu minimieren. 
Trotz der Automatisierung vieler standardisierter Aufgaben bleiben menschliche Übersetzer*innen unverzichtbar, insbesondere für die Übermittlung kultureller Nuancen und kreativer Ausdrucksformen. Je mehr Maschinen das \glqq{}Übersetzbare\grqq{} bewältigen, desto wichtiger wird es für Übersetzerinnen, das \glqq{}Unübersetzbare\grqq{} zu vermitteln. Der Schlüssel liegt in kontinuierlicher Weiterbildung und gezieltem Einsatz neuer Technologien, um die eigene Expertise zu erweitern. 
In Übersetzungsabteilungen wird zunehmend diskutiert, wie Technologien wie NMT und KI in die Lehre integriert werden können. Sollten sie für Aufgaben, Prüfungen oder Abschlussarbeiten zulässig sein? Wie können sie genutzt werden, um den Übersetzungsprozess zu unterstützen und die Kompetenzen von Studierenden zu fördern? Die Einführung von NMT und KI hat eine Bandbreite an Reaktionen ausgelöst: von Begeisterung bis Besorgnis. Einerseits versprechen sie Produktivitätssteigerung und höhere Qualität, andererseits besteht die Sorge, dass klassische Übersetzungsaufgaben überflüssig werden könnten. Einige meinen Studierende sollten in ihrem Studium möglichst früh mit diesen Technologien vertraut gemacht werden, während andere meinen, vor dem kritischen Umgang mit MT und LLMs sollten zuerst die menschlichen Übersetzungsfähigkeiten geschult und entfaltet werden, um sich nicht blindlings auf MT und KI zu verlassen.
Es besteht Einigkeit darüber, dass der Umgang mit MT und LLMs eine Kernkompetenz darstellt. Statt diese Technologien lediglich zu erlauben oder zu verbieten, sollte ihre Anwendung aktiv in die Lehre integriert werden, um ihre Potenziale zu nutzen und mögliche negative Auswirkungen zu minimieren.

 


%\section*{Acknowledgements}
% \citet{Nordhoff2018} is useful for compiling bibliographies
% %\section*{Contributions}
% %John Doe contributed to conceptualization, methodology, and validation. 
% %Jane Doe contributed to writing of the original draft, review, and editing.

\sloppy
\printbibliography[heading=subbibliography,notkeyword=this]
\end{document}
